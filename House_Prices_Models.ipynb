{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "from datetime import datetime\n",
    "\n",
    "# Define the notebook title\n",
    "Notebook_title = \"House_Prices_Models\"\n",
    "\n",
    "# Get the current date\n",
    "current_date = datetime.now().strftime(\"%B %d, %Y\")\n",
    "\n",
    "# Create the HTML string with title, date, and author\n",
    "html_content = f\"\"\"\n",
    "<h1 style=\"text-align:center;\">{Notebook_title}</h1>\n",
    "<br/>\n",
    "<h3 style=\"text-align:left;\">MikiasHWT</h3>\n",
    "<h3 style=\"text-align:left;\">{current_date}</h3>\n",
    "\"\"\"\n",
    "\n",
    "# Display the HTML content in the output\n",
    "display(HTML(html_content))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prep Workplace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "from setuptools import distutils\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n",
    "\n",
    "# To show multiple lines in output\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directories = {\n",
    "#     \"datDir\": \"path/\",\n",
    "#     \"outDir\": \"path/\"\n",
    "# }\n",
    "\n",
    "# # Data Directory\n",
    "# datDir = \"\"\n",
    "\n",
    "# # View data\n",
    "# os.listdir(datDir)\n",
    "\n",
    "# # Output Directory\n",
    "# outDir = \"\"\n",
    "\n",
    "# # View outputs\n",
    "# os.listdir(outDir)\n",
    "\n",
    "# # Define required directories\n",
    "# Directories = {\n",
    "#     \"datDir\": \"path/to/data\",\n",
    "#     \"outDir\": \"path/to/output\"\n",
    "# }\n",
    "\n",
    "# # Create directories (if they don't exist) & list contents\n",
    "# for folder, path in Directories.items():\n",
    "#     os.makedirs(path, exist_ok=True) \n",
    "#     print(f\"Contents of {path}:\")\n",
    "#     print(\"\\n\".join(os.listdir(path))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DatDescriptionPath = 'data/data_description.txt'\n",
    "\n",
    "# DatDescription = pd.read_csv('data/data_description.txt', delimiter=' ')\n",
    "\n",
    "\n",
    "# Open and read the text file\n",
    "with open(DatDescriptionPath, 'r') as file:\n",
    "    content = file.read()\n",
    "\n",
    "# Display the content\n",
    "print(content)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define filepaths\n",
    "iowa_file_path = 'data/train.csv'\n",
    "test_data_path = 'data/test.csv'\n",
    "\n",
    "# Read data files\n",
    "HomeDat = pd.read_csv(iowa_file_path, index_col=\"Id\")\n",
    "TestDat = pd.read_csv(test_data_path, index_col=\"Id\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variable types\n",
    "variable_types = HomeDat.dtypes\n",
    "\n",
    "# Summary\n",
    "variable_types.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics: numerical variables\n",
    "summary_stats = HomeDat.describe()\n",
    "\n",
    "summary_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics: categorical variables\n",
    "categorical_summary = HomeDat.describe(include='object')\n",
    "\n",
    "categorical_summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Categorize Features\n",
    "- Numerical Variables: Continuous and discrete numbers.\n",
    "- Categorical Variables: Variables with distinct categories.\n",
    "- Date/Time Variables: Variables representing dates or times (if applicable).\n",
    "- Target Variable: The variable you are trying to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate numerical and categorical columns\n",
    "numerical_columns = HomeDat.select_dtypes(include=['number']).columns\n",
    "categorical_columns = HomeDat.select_dtypes(include=['object', 'category']).columns\n",
    "\n",
    "print(f\"Numerical columns: {numerical_columns} \\n {len(numerical_columns)} columns total\")\n",
    "print(f\"\\nCategorical columns: {categorical_columns} \\n {len(categorical_columns)} columns total\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate predictor and outcome variables (using only numerical predictors for now)\n",
    "X = HomeDat[numerical_columns.drop(\"SalePrice\")]    \n",
    "X.shape\n",
    "\n",
    "y = HomeDat[\"SalePrice\"]\n",
    "y.shape\n",
    "\n",
    "# Prep train and test data\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, \n",
    "                                                  train_size=0.8, \n",
    "                                                  random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handle Missing Values\n",
    "- Impute \n",
    "- Remove\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "missing_values_all = HomeDat.isnull().sum().sort_values(ascending=False)\n",
    "print(f\"Number of missing values for all columns: \\n{missing_values_all[missing_values_all> 0]}\")\n",
    "\n",
    "missing_numeric_values = X.isnull().sum().sort_values(ascending=False)\n",
    "print(f\"\\nNumber of missing values for numeric columns: \\n{missing_numeric_values[missing_numeric_values> 0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of columns missing values\n",
    "print(f\"There are {(missing_numeric_values > 0).sum()} numerical variables containing NA's.\")\n",
    "print(f\"For a total of {missing_numeric_values.sum()} missing numeric values.\")\n",
    "\n",
    "\n",
    "# Context\n",
    "print(f\"\\nThe whole dataset has {(missing_values_all > 0).sum()} variables containing NA's. \\nFor a sum total of {missing_values_all.sum()} missing values\")\n",
    "print(f\"\\nThe whole dataset contains {HomeDat.shape[0]} observations and {HomeDat.shape[1]} variables.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compare various Random Forest models\n",
    "def score_dataset(X_train, X_val, y_train, y_val):\n",
    "    model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_val)\n",
    "    return mean_absolute_error(y_val, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Names of columns with missing values\n",
    "cols_with_nulls = [col for col in X_train.columns\n",
    "                     if X_train[col].isnull().any()]\n",
    "\n",
    "# Drop columns missing values\n",
    "reduced_X_train = X_train.drop(columns = cols_with_nulls, axis=1)\n",
    "reduced_X_valid = X_val.drop(columns = cols_with_nulls, axis=1)\n",
    "\n",
    "print(f\"For one approach, we eliminate columns with NA values, reducing our datasets from {X_train.shape[1]} to {reduced_X_train.shape[1]} variables.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"MAE (Drop columns with missing values):\")\n",
    "print(score_dataset(reduced_X_train, reduced_X_valid, y_train, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Impute values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create instance of imputer class\n",
    "my_imputer = SimpleImputer()\n",
    "\n",
    "# Impute missing values in train data with mean value of the column (mean = default)\n",
    "imputed_X_train = pd.DataFrame(my_imputer.fit_transform(X_train))\n",
    "\n",
    "# Use train data mean to impute missing values in validation dataset (transform without fit for consistency)\n",
    "imputed_X_valid = pd.DataFrame(my_imputer.transform(X_val))\n",
    "\n",
    "# Numpy array is returned without column names, return the names\n",
    "imputed_X_train.columns = X_train.columns\n",
    "imputed_X_valid.columns = X_val.columns\n",
    "\n",
    "print(\"MAE from Approach 2 (SimpleImputer):\")\n",
    "print(score_dataset(imputed_X_train, imputed_X_valid, y_train, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results are worse for imputing values that dropping columns. \n",
    "This result can be expected depending on the nature of the dataset, the variables we imputed as well as how we chose to impute their values. \n",
    "\n",
    "eg: Missing values in GarageYrBlt (Year garage was built) variable could indicated the house doesn't have a garage. So imputing a mean could produce more noise in the dataset. In this case, an alternate approach may be warranted. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Garage Variables\n",
    "- GarageYrBlt (Year garage was built)\n",
    "- GarageType: Garage location\n",
    "\t\t\n",
    "       2Types\tMore than one type of garage\n",
    "       Attchd\tAttached to home\n",
    "       Basment\tBasement Garage\n",
    "       BuiltIn\tBuilt-In (Garage part of house - typically has room above garage)\n",
    "       CarPort\tCar Port\n",
    "       Detchd\tDetached from home\n",
    "       NA\tNo Garage\n",
    "\t\t\n",
    "- GarageYrBlt: Year garage was built\n",
    "\t\t\n",
    "- GarageFinish: Interior finish of the garage\n",
    "\n",
    "       Fin\tFinished\n",
    "       RFn\tRough Finished\t\n",
    "       Unf\tUnfinished\n",
    "       NA\tNo Garage\n",
    "\t\t\n",
    "- GarageCars: Size of garage in car capacity\n",
    "\n",
    "- GarageArea: Size of garage in square feet\n",
    "\n",
    "- GarageQual: Garage quality\n",
    "\n",
    "       Ex\tExcellent\n",
    "       Gd\tGood\n",
    "       TA\tTypical/Average\n",
    "       Fa\tFair\n",
    "       Po\tPoor\n",
    "       NA\tNo Garage\n",
    "\t\t\n",
    "- GarageCond: Garage condition\n",
    "\n",
    "       Ex\tExcellent\n",
    "       Gd\tGood\n",
    "       TA\tTypical/Average\n",
    "       Fa\tFair\n",
    "       Po\tPoor\n",
    "       NA\tNo Garage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract garage related variables from full dataset\n",
    "garage_columns = HomeDat.filter(like=\"Garage\")[HomeDat['GarageYrBlt'].isnull()]\n",
    "\n",
    "# Explore \n",
    "garage_columns.shape\n",
    "\n",
    "print(f\"These Garage related variables have the following number of NA values: \\n{garage_columns.isnull().sum()}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consider value of GarageYrBlt columns\n",
    "# HomeDat[[\"GarageYrBlt\", \"GarageArea\", \"YearBuilt\", \"YearRemodAdd\"]]\n",
    "\n",
    "HomeDat[[\"GarageYrBlt\", \"YearBuilt\", \"YearRemodAdd\"]].describe()\n",
    "\n",
    "# Correlation of potentially related variables\n",
    "HomeDat[[\"GarageArea\", \"GarageYrBlt\", \"YearBuilt\", \"YearRemodAdd\"]].corr()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove NaN values from the data\n",
    "filtered_data = HomeDat[['GarageYrBlt', 'YearBuilt']]\n",
    "\n",
    "# Scatter plot with best-fit line\n",
    "sns.lmplot(x='GarageYrBlt', y='YearBuilt', data=filtered_data, line_kws={'color': 'magenta'})\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_subset = HomeDat[[\"GarageYrBlt\", \"GarageArea\", \"YearBuilt\", \"YearRemodAdd\"]]\n",
    "target = HomeDat[\"SalePrice\"]  # Replace 'SalePrice' with your target column\n",
    "\n",
    "model = DecisionTreeRegressor()\n",
    "model.fit(features_subset, target)\n",
    "\n",
    "# Feature importance\n",
    "feature_importance = model.feature_importances_\n",
    "\n",
    "# Create a DataFrame to view feature importance\n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': [\"GarageYrBlt\", \"GarageArea\", \"YearBuilt\", \"YearRemodAdd\"],\n",
    "    'Importance': feature_importance\n",
    "})\n",
    "\n",
    "importance_df.sort_values(by=\"Importance\", ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision: Drop GarageYrBlt Variable. \n",
    "- Comparing garage related variables, it seems null values in GarageYrBlt indicate homes without garages. This is evident by the area's given as 0 and by NA values for other columns.\n",
    "- GarageYrBlt and YearBuilt are highly correlated (82%)\n",
    "- Quick inspection of the importance of GarageYrBlt (relative to the 3 other) variable shows minimal significance of the variable to the Random Forest model created. (same with YearRemodAdd) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LotFrontage Variables (-ish)\n",
    "- LotFrontage: Linear feet of street connected to property\n",
    "\n",
    "- LotArea: Lot size in square feet\n",
    "\t\n",
    "- LotShape: General shape of property\n",
    "\n",
    "       Reg\tRegular\t\n",
    "       IR1\tSlightly irregular\n",
    "       IR2\tModerately Irregular\n",
    "       IR3\tIrregular\n",
    "\t\n",
    "- LotConfig: Lot configuration\n",
    "\n",
    "       Inside\tInside lot\n",
    "       Corner\tCorner lot\n",
    "       CulDSac\tCul-de-sac\n",
    "       FR2\tFrontage on 2 sides of property\n",
    "       FR3\tFrontage on 3 sides of property"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract lot related variables from full dataset\n",
    "lot_columns = HomeDat.filter(like=\"Lot\")[HomeDat['LotFrontage'].isnull()]\n",
    "\n",
    "lot_columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LotFrontage by LotConfig \n",
    "lotconfig_summary = HomeDat.groupby('LotConfig')['LotFrontage'].agg(\n",
    "    Total_Instances='size',\n",
    "    NA_Count=lambda x: x.isnull().sum(),\n",
    "    Average_LotFrontage='mean'\n",
    ")\n",
    "\n",
    "# LotFrontage by LotShape\n",
    "lotshape_summary = HomeDat.groupby('LotShape')['LotFrontage'].agg(\n",
    "    Total_Instances='size',\n",
    "    NA_Count=lambda x: x.isnull().sum(),\n",
    "    Average_LotFrontage='mean'\n",
    ")\n",
    "\n",
    "# Display summaries\n",
    "print(\"LotFrontage missing values and averages by LotConfig:\")\n",
    "print(lotconfig_summary)\n",
    "\n",
    "print(\"\\nLotFrontage missing values and averages by LotShape:\")\n",
    "print(lotshape_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HomeDat[['LotFrontage', 'LotArea']].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a figure and adjust its size\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "# Plot 1: Scatter plot with best-fit lines colored by LotShape\n",
    "sns.lmplot(x='LotArea', y='LotFrontage', hue='LotShape', data=HomeDat, scatter_kws={'s': 10}, line_kws={'linewidth': 2}, ci=None)\n",
    "plt.title('LotArea vs LotFrontage by LotShape')\n",
    "\n",
    "# Plot 2: Scatter plot with best-fit lines colored by LotConfig\n",
    "sns.lmplot(x='LotArea', y='LotFrontage', hue='LotConfig', data=HomeDat, scatter_kws={'s': 10}, line_kws={'linewidth': 2}, ci=None)\n",
    "plt.title('LotArea vs LotFrontage by LotConfig')\n",
    "\n",
    "# Adjust layout of plots (shown in Jupyter Notebook)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_subset2 = HomeDat[['LotFrontage', 'LotArea']]\n",
    "target = HomeDat[\"SalePrice\"]  # Replace 'SalePrice' with your target column\n",
    "\n",
    "model = DecisionTreeRegressor()\n",
    "model.fit(features_subset2, target)\n",
    "\n",
    "# Feature importance\n",
    "feature_importance2 = model.feature_importances_\n",
    "\n",
    "# Create a DataFrame to view feature importance\n",
    "importance_df2 = pd.DataFrame({\n",
    "    'Feature': ['LotFrontage', 'LotArea'],\n",
    "    'Importance': feature_importance2\n",
    "})\n",
    "\n",
    "importance_df2.sort_values(by=\"Importance\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_subset3 = HomeDat[numerical_columns.drop(\"SalePrice\")] \n",
    "target = HomeDat[\"SalePrice\"]  # Replace 'SalePrice' with your target column\n",
    "\n",
    "model = DecisionTreeRegressor()\n",
    "model.fit(features_subset3, target)\n",
    "\n",
    "# Feature importance\n",
    "feature_importance3 = model.feature_importances_\n",
    "\n",
    "# Create a DataFrame to view feature importance\n",
    "importance_df3 = pd.DataFrame({\n",
    "    'Feature': numerical_columns.drop(\"SalePrice\"),\n",
    "    'Importance': feature_importance3\n",
    "})\n",
    "\n",
    "importance_df3.sort_values(by=\"Importance\", ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision: Impute LotFrontage with median values\n",
    "- I see no obvious patters to lead me to impute a category specific value for LotFrontage\n",
    "- I see no strong patterns between lot variables and missing LotFrontage values\n",
    "- I am not convinced that LotFrontage is insignificant enough to discard\n",
    "- I believe imputing with a median value will cause the least induction of noise, while preserving the potentially utility of LotFrontage variable. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MasVnrArea Variables\n",
    "\n",
    "- MasVnrArea: Masonry veneer area in square feet\n",
    "\n",
    "- MasVnrType: Masonry veneer type\n",
    "\n",
    "       BrkCmn\tBrick Common\n",
    "       BrkFace\tBrick Face\n",
    "       CBlock\tCinder Block\n",
    "       None\tNone\n",
    "       Stone\tStone\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract garage related variables from full dataset\n",
    "veneer_columns = HomeDat.filter(like=\"Vnr\")[(HomeDat['MasVnrArea'].isnull()) | (HomeDat['MasVnrType'] == \"None\")]\n",
    "\n",
    "# Explore \n",
    "veneer_columns\n",
    "\n",
    "# Count instances of each value in MasVnrType\n",
    "mas_vnr_type_counts = HomeDat['MasVnrType'].value_counts()\n",
    "\n",
    "# Display the counts\n",
    "print(mas_vnr_type_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision: Impute MasVnrArea with 0\n",
    "- The instances of MasVnrType are missing CinderBlock and None. I believe in this case, it is fair to assume there is no masonry for these observations and impute a value of zero in their stead. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictor subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop GarageYrBlt column \n",
    "X_train_sub1 = X_train.drop(columns = \"GarageYrBlt\", axis=1)\n",
    "X_valid_sub1 = X_val.drop(columns = \"GarageYrBlt\", axis=1)\n",
    "\n",
    "# Compare\n",
    "print(f\"Original data shape:{X_train.shape}. \\nReduced data shape:{X_train_sub1.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy data\n",
    "X_train_sub1_alt = X_train.copy()\n",
    "X_valid_sub1_alt = X_val.copy()\n",
    "\n",
    "# Create impute key (1 = imputed, 0 = not imputed) for training and validation sets\n",
    "X_train_sub1_alt['GarageYrBlt_Imputed'] = X_train_sub1_alt['GarageYrBlt'].isnull().astype(int)\n",
    "X_valid_sub1_alt['GarageYrBlt_Imputed'] = X_valid_sub1_alt['GarageYrBlt'].isnull().astype(int)\n",
    "\n",
    "# Impute missing GarageYrBlt with corresponding YearBuilt in training and validation sets\n",
    "X_train_sub1_alt['GarageYrBlt'] = X_train_sub1_alt['GarageYrBlt'].fillna(X_train_sub1_alt['YearBuilt'])\n",
    "X_valid_sub1_alt['GarageYrBlt'] = X_valid_sub1_alt['GarageYrBlt'].fillna(X_valid_sub1_alt['YearBuilt'])\n",
    "\n",
    "# Show impact of imputed variable (training data)\n",
    "print(pd.concat([X_train['GarageYrBlt'].describe(), X_train_sub1_alt['GarageYrBlt'].describe()], axis=1).round(2).set_axis(['Original GarageYrBlt', 'Imputed GarageYrBlt'], axis=1))\n",
    "\n",
    "# Print the impact on NAN values\n",
    "print(f\"\\n{X_train['GarageYrBlt'].isnull().sum()} previously missing values have been imputed using YearBuilt\")\n",
    "\n",
    "# Print shape of original and imputed data\n",
    "print(f\"\\nOriginal data shape: {X_train.shape}. \\nImputed data shape: {X_train_sub1_alt.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy data\n",
    "X_train_sub2 = X_train_sub1.copy()\n",
    "X_valid_sub2 = X_valid_sub1.copy()\n",
    "\n",
    "# Median LofFrontage\n",
    "lot_frontage_median = X_train_sub2['LotFrontage'].median()\n",
    "\n",
    "# Create impute key (1 = imputed, 0 = not imputed) \n",
    "X_train_sub2['LotFrontage_Imputed'] = X_train_sub2['LotFrontage'].isnull().astype(int)\n",
    "X_valid_sub2['LotFrontage_Imputed'] = X_valid_sub2['LotFrontage'].isnull().astype(int)\n",
    "\n",
    "# Impute missing LotFrontage with median\n",
    "X_train_sub2['LotFrontage'] = X_train_sub2['LotFrontage'].fillna(lot_frontage_median)\n",
    "X_valid_sub2['LotFrontage'] = X_valid_sub2['LotFrontage'].fillna(lot_frontage_median)\n",
    "\n",
    "# Show impact of imputed variable\n",
    "print(pd.concat([X_train_sub1['LotFrontage'].describe(), X_train_sub2['LotFrontage'].describe()], axis=1).round(2).set_axis(['Original LotFrontage', 'Median Imputed LotFrontage'], axis=1))\n",
    "\n",
    "print(f\"\\n{X_train['LotFrontage'].isnull().sum()} previously NAN values added to imputed counts, with minimal impact on other statistics\")\n",
    "print(f\"\\nOriginal data shape:{X_train_sub1.shape}. \\nImputed data shape:{X_train_sub2.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy data\n",
    "X_train_sub2_alt = X_train_sub1_alt.copy()\n",
    "X_valid_sub2_alt = X_valid_sub1_alt.copy()\n",
    "\n",
    "# Median LofFrontage\n",
    "lot_frontage_median = X_train_sub2_alt['LotFrontage'].median()\n",
    "\n",
    "# Create impute key (1 = imputed, 0 = not imputed) \n",
    "X_train_sub2_alt['LotFrontage_Imputed'] = X_train_sub2_alt['LotFrontage'].isnull().astype(int)\n",
    "X_valid_sub2_alt['LotFrontage_Imputed'] = X_valid_sub2_alt['LotFrontage'].isnull().astype(int)\n",
    "\n",
    "# Impute missing LotFrontage with median\n",
    "X_train_sub2_alt['LotFrontage'] = X_train_sub2_alt['LotFrontage'].fillna(lot_frontage_median)\n",
    "X_valid_sub2_alt['LotFrontage'] = X_valid_sub2_alt['LotFrontage'].fillna(lot_frontage_median)\n",
    "\n",
    "# Show impact of imputed variable\n",
    "print(pd.concat([X_train_sub1_alt['LotFrontage'].describe(), X_train_sub2_alt['LotFrontage'].describe()], axis=1).round(2).set_axis(['Original LotFrontage', 'Median Imputed LotFrontage'], axis=1))\n",
    "\n",
    "print(f\"\\n{X_train['LotFrontage'].isnull().sum()} previously NAN values added to imputed counts, with minimal impact on other statistics\")\n",
    "print(f\"\\nOriginal data shape:{X_train_sub1_alt.shape}. \\nImputed data shape:{X_train_sub2_alt.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy data\n",
    "X_train_Lot = X_train.copy()\n",
    "X_valid_Lot = X_val.copy()\n",
    "\n",
    "# Median LofFrontage\n",
    "lot_frontage_median = X_train_Lot['LotFrontage'].median()\n",
    "\n",
    "# Create impute key (1 = imputed, 0 = not imputed) \n",
    "X_train_Lot['LotFrontage_Imputed'] = X_train_Lot['LotFrontage'].isnull().astype(int)\n",
    "X_valid_Lot['LotFrontage_Imputed'] = X_valid_Lot['LotFrontage'].isnull().astype(int)\n",
    "\n",
    "# Impute missing LotFrontage with median\n",
    "X_train_Lot['LotFrontage'] = X_train_Lot['LotFrontage'].fillna(lot_frontage_median)\n",
    "X_valid_Lot['LotFrontage'] = X_valid_Lot['LotFrontage'].fillna(lot_frontage_median)\n",
    "\n",
    "# Show impact of imputed variable\n",
    "print(pd.concat([X_train['LotFrontage'].describe(), X_train_Lot['LotFrontage'].describe()], axis=1).round(2).set_axis(['Original LotFrontage', 'Median Imputed LotFrontage'], axis=1))\n",
    "\n",
    "print(f\"\\n{X_train['LotFrontage'].isnull().sum()} previously NAN values added to imputed counts, with minimal impact on other statistics\")\n",
    "print(f\"\\nOriginal data shape:{X_train.shape}. \\nImputed data shape:{X_train_Lot.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy data\n",
    "X_train_Lot_nokey = X_train.copy()\n",
    "X_valid_Lot_nokey = X_val.copy()\n",
    "\n",
    "# Median LofFrontage\n",
    "lot_frontage_median = X_train_Lot_nokey['LotFrontage'].median()\n",
    "\n",
    "# Impute missing LotFrontage with median\n",
    "X_train_Lot_nokey['LotFrontage'] = X_train_Lot_nokey['LotFrontage'].fillna(lot_frontage_median)\n",
    "X_valid_Lot_nokey['LotFrontage'] = X_valid_Lot_nokey['LotFrontage'].fillna(lot_frontage_median)\n",
    "\n",
    "# Show impact of imputed variable\n",
    "print(pd.concat([X_train['LotFrontage'].describe(), X_train_Lot_nokey['LotFrontage'].describe()], axis=1).round(2).set_axis(['Original LotFrontage', 'Median Imputed LotFrontage'], axis=1))\n",
    "\n",
    "print(f\"\\n{X_train['LotFrontage'].isnull().sum()} previously NAN values added to imputed counts, with minimal impact on other statistics\")\n",
    "print(f\"\\nOriginal data shape:{X_train.shape}. \\nImputed data shape:{X_train_Lot_nokey.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy data\n",
    "X_train_Lot0_nokey = X_train.copy()\n",
    "X_valid_Lot0_nokey = X_val.copy()\n",
    "\n",
    "# Impute missing values in MasVnrArea with 0\n",
    "X_train_Lot0_nokey['LotFrontage'] = X_train_Lot0_nokey['LotFrontage'].fillna(0)\n",
    "X_valid_Lot0_nokey['LotFrontage'] = X_valid_Lot0_nokey['LotFrontage'].fillna(0)\n",
    "\n",
    "# Show impact\n",
    "print(pd.concat([X_train['LotFrontage'].describe(), X_train_Lot0_nokey['LotFrontage'].describe()], axis=1).round(2).set_axis(['Original LotFrontage', 'Zero Imputed LotFrontage'], axis=1))\n",
    "\n",
    "print(f\"\\n{X_train['LotFrontage'].isnull().sum()} previously NAN values added to imputed counts\")\n",
    "print(f\"\\nOriginal data shape:{X_train.shape}. \\nImputed data shape:{X_train_Lot0_nokey.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy data\n",
    "X_train_sub3 = X_train_sub2.copy()\n",
    "X_valid_sub3 = X_valid_sub2.copy()\n",
    "\n",
    "# Create imputation key (1 = imputed, 0 = not imputed)\n",
    "X_train_sub3['MasVnrArea_Imputed'] = X_train_sub3['MasVnrArea'].isnull().astype(int)\n",
    "X_valid_sub3['MasVnrArea_Imputed'] = X_valid_sub3['MasVnrArea'].isnull().astype(int)\n",
    "\n",
    "# Impute missing values in MasVnrArea with 0\n",
    "X_train_sub3['MasVnrArea'] = X_train_sub3['MasVnrArea'].fillna(0)\n",
    "X_valid_sub3['MasVnrArea'] = X_valid_sub3['MasVnrArea'].fillna(0)\n",
    "\n",
    "# Show impact\n",
    "print(pd.concat([X_train_sub2['MasVnrArea'].describe(), X_train_sub3['MasVnrArea'].describe()], axis=1).round(2).set_axis(['Original MasVnrArea', 'Zero Imputed MasVnrArea'], axis=1))\n",
    "\n",
    "print(f\"\\n{X_train['MasVnrArea'].isnull().sum()} previously NAN values added to imputed counts, with minimal impact on other statistics\")\n",
    "print(f\"\\nOriginal data shape:{X_train_sub2.shape}. \\nImputed data shape:{X_train_sub3.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy data\n",
    "X_train_sub3_alt = X_train_sub2_alt.copy()\n",
    "X_valid_sub3_alt = X_valid_sub2_alt.copy()\n",
    "\n",
    "# Create imputation key (1 = imputed, 0 = not imputed)\n",
    "X_train_sub3_alt['MasVnrArea_Imputed'] = X_train_sub3_alt['MasVnrArea'].isnull().astype(int)\n",
    "X_valid_sub3_alt['MasVnrArea_Imputed'] = X_valid_sub3_alt['MasVnrArea'].isnull().astype(int)\n",
    "\n",
    "# Impute missing values in MasVnrArea with 0\n",
    "X_train_sub3_alt['MasVnrArea'] = X_train_sub3_alt['MasVnrArea'].fillna(0)\n",
    "X_valid_sub3_alt['MasVnrArea'] = X_valid_sub3_alt['MasVnrArea'].fillna(0)\n",
    "\n",
    "# Show impact\n",
    "print(pd.concat([X_train_sub2_alt['MasVnrArea'].describe(), X_train_sub3_alt['MasVnrArea'].describe()], axis=1).round(2).set_axis(['Original MasVnrArea', 'Zero Imputed MasVnrArea'], axis=1))\n",
    "\n",
    "print(f\"\\n{X_train['MasVnrArea'].isnull().sum()} previously NAN values added to imputed counts, with minimal impact on other statistics\")\n",
    "print(f\"\\nOriginal data shape:{X_train_sub2_alt.shape}. \\nImputed data shape:{X_train_sub3_alt.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy data\n",
    "X_train_Vnr = X_train.copy()\n",
    "X_valid_Vnr = X_val.copy()\n",
    "\n",
    "# Create imputation key (1 = imputed, 0 = not imputed)\n",
    "X_train_Vnr['MasVnrArea_Imputed'] = X_train_Vnr['MasVnrArea'].isnull().astype(int)\n",
    "X_valid_Vnr['MasVnrArea_Imputed'] = X_valid_Vnr['MasVnrArea'].isnull().astype(int)\n",
    "\n",
    "# Impute missing values in MasVnrArea with 0\n",
    "X_train_Vnr['MasVnrArea'] = X_train_Vnr['MasVnrArea'].fillna(0)\n",
    "X_valid_Vnr['MasVnrArea'] = X_valid_Vnr['MasVnrArea'].fillna(0)\n",
    "\n",
    "# Show impact\n",
    "print(pd.concat([X_train['MasVnrArea'].describe(), X_train_Vnr['MasVnrArea'].describe()], axis=1).round(2).set_axis(['Original MasVnrArea', 'Zero Imputed MasVnrArea'], axis=1))\n",
    "\n",
    "print(f\"\\n{X_train['MasVnrArea'].isnull().sum()} previously NAN values added to imputed counts, with minimal impact on other statistics\")\n",
    "print(f\"\\nOriginal data shape:{X_train.shape}. \\nImputed data shape:{X_train_Vnr.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy data\n",
    "X_train_Vnr_nokey = X_train.copy()\n",
    "X_valid_Vnr_nokey = X_val.copy()\n",
    "\n",
    "# Impute missing values in MasVnrArea with 0\n",
    "X_train_Vnr_nokey['MasVnrArea'] = X_train_Vnr_nokey['MasVnrArea'].fillna(0)\n",
    "X_valid_Vnr_nokey['MasVnrArea'] = X_valid_Vnr_nokey['MasVnrArea'].fillna(0)\n",
    "\n",
    "# Show impact\n",
    "print(pd.concat([X_train['MasVnrArea'].describe(), X_train_Vnr_nokey['MasVnrArea'].describe()], axis=1).round(2).set_axis(['Original MasVnrArea', 'Zero Imputed MasVnrArea'], axis=1))\n",
    "\n",
    "print(f\"\\n{X_train['MasVnrArea'].isnull().sum()} previously NAN values added to imputed counts, with minimal impact on other statistics\")\n",
    "print(f\"\\nOriginal data shape:{X_train.shape}. \\nImputed data shape:{X_train_Vnr_nokey.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"MAE from Approach: Original\")\n",
    "print(score_dataset(X_train, X_val, y_train, y_val))\n",
    "\n",
    "print(\"\\nMAE (Drop columns with missing values):\")\n",
    "print(score_dataset(reduced_X_train, reduced_X_valid, y_train, y_val))\n",
    "\n",
    "print(\"\\nMAE from Approach 2 (SimpleImputer):\")\n",
    "print(score_dataset(imputed_X_train, imputed_X_valid, y_train, y_val))\n",
    "\n",
    "print(\"______________________________________________________________________\")\n",
    "\n",
    "print(\"\\nMAE from Approach: GarageYrBlt Dropped\")\n",
    "print(score_dataset(X_train_sub1, X_valid_sub1, y_train, y_val))\n",
    "\n",
    "print(\"\\nMAE from Approach: Mediain Imputed LotFrontage with key\")\n",
    "print(score_dataset(X_train_sub2, X_valid_sub2, y_train, y_val))\n",
    "\n",
    "print(\"\\nMAE from Approach: Zero Imputed MasVnrArea with key\")\n",
    "print(score_dataset(X_train_sub3, X_valid_sub3, y_train, y_val))\n",
    "\n",
    "print(\"______________________________________________________________________\")\n",
    "\n",
    "print(\"\\nMAE from Alt Approach: YearBlt Imputed GarageYrBlt with key\")\n",
    "print(score_dataset(X_train_sub1_alt, X_valid_sub1_alt, y_train, y_val))\n",
    "\n",
    "print(\"\\nMAE from Alt Approach: Mediain Imputed LotFrontage with key\")\n",
    "print(score_dataset(X_train_sub2_alt, X_valid_sub2_alt, y_train, y_val))\n",
    "\n",
    "print(\"\\nMAE from Alt Approach: Zero Imputed MasVnrArea with key\")\n",
    "print(score_dataset(X_train_sub3_alt, X_valid_sub3_alt, y_train, y_val))\n",
    "\n",
    "print(\"______________________________________________________________________\")\n",
    "\n",
    "print(\"\\nMAE from Lot Only Approach: Mediain Imputed LotFrontage with key\")\n",
    "print(score_dataset(X_train_Lot, X_valid_Lot, y_train, y_val))\n",
    "\n",
    "print(\"\\nMAE from Vnr Only Approach: Zero Imputed MasVnrArea with key\")\n",
    "print(score_dataset(X_train_Vnr, X_valid_Vnr, y_train, y_val))\n",
    "\n",
    "print(\"______________________________________________________________________\")\n",
    "\n",
    "print(\"\\nMAE from Lot Only NO-KEY Approach: Mediain Imputed LotFrontage without key\")\n",
    "print(score_dataset(X_train_Lot_nokey, X_valid_Lot_nokey, y_train, y_val))\n",
    "\n",
    "print(\"\\nMAE from Lot Only NO-KEY Approach: Zero Imputed LotFrontage without key\")\n",
    "print(score_dataset(X_train_Lot0_nokey, X_valid_Lot0_nokey, y_train, y_val))\n",
    "\n",
    "print(\"\\nMAE from Vnr Only NO-KEY Approach: Zero Imputed MasVnrArea with key\")\n",
    "print(score_dataset(X_train_Vnr_nokey, X_valid_Vnr_nokey, y_train, y_val))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Categorical Variables\n",
    "- Drop variables\n",
    "- Ordinal Variables: 1-N per tier of ordered categorical variable (Ordinal encoding)\n",
    "- Nominal Variable: one column of binary options per tier of categorical variable (one-hot encoding)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Drop columns with missing values (simplest approach)\n",
    "# cols_with_missing = [col for col in X_train.columns if X_train[col].isnull().any()] \n",
    "# X_train.drop(cols_with_missing, axis=1, inplace=True)\n",
    "# X_valid.drop(cols_with_missing, axis=1, inplace=True)\n",
    "\n",
    "# # \"Cardinality\" means the number of unique values in a column\n",
    "# # Select categorical columns with relatively low cardinality (convenient but arbitrary)\n",
    "# low_cardinality_cols = [cname for cname in X_train.columns if X_train[cname].nunique() < 10 and X_train[cname].dtype == \"object\"]\n",
    "\n",
    "\n",
    "# # Select numerical columns\n",
    "# numerical_cols = [cname for cname in X_train.columns if X_train[cname].dtype in ['int64', 'float64']]\n",
    "\n",
    "# # Keep selected columns only\n",
    "# my_cols = low_cardinality_cols + numerical_cols\n",
    "# X_train = X_train_full[my_cols].copy()\n",
    "# X_valid = X_valid_full[my_cols].copy()\n",
    "\n",
    "# # Get list of categorical variables\n",
    "# s = (X_train.dtypes == 'object')\n",
    "# object_cols = list(s[s].index)\n",
    "\n",
    "# print(\"Categorical variables:\")\n",
    "# print(object_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Look at unique values in a specified columns\n",
    "\n",
    "# print(\"Unique values in 'Condition2' column in training data:\", X_train['Condition2'].unique())\n",
    "# print(\"\\nUnique values in 'Condition2' column in validation data:\", X_valid['Condition2'].unique())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bad Categorical variables \n",
    "Fitting an ordinal encoder to a column in the training data creates a corresponding integer-valued label for each unique value that appears in the training data. In the case that the validation data contains values that don't also appear in the training data, the encoder will throw an error, because these values won't have an integer assigned to them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Categorical columns in the training data\n",
    "# object_cols = [col for col in X_train.columns if X_train[col].dtype == \"object\"]\n",
    "\n",
    "# # Columns that can be safely ordinal encoded\n",
    "# good_label_cols = [col for col in object_cols if \n",
    "#                    set(X_valid[col]).issubset(set(X_train[col]))]\n",
    "        \n",
    "# # Problematic columns that will be dropped from the dataset\n",
    "# bad_label_cols = list(set(object_cols)-set(good_label_cols))\n",
    "        \n",
    "# print('Categorical columns that will be ordinal encoded:', good_label_cols)\n",
    "# print('\\nCategorical columns that will be dropped from the dataset:', bad_label_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Drop\n",
    "# drop_X_train = X_train.select_dtypes(exclude=['object'])\n",
    "# drop_X_valid = X_valid.select_dtypes(exclude=['object'])\n",
    "\n",
    "# print(\"MAE from Approach 1 (Drop categorical variables):\")\n",
    "# print(score_dataset(drop_X_train, drop_X_valid, y_train, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ordinal Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "# Ordinal Encoding\n",
    "\n",
    "# Make copy to avoid changing original data \n",
    "label_X_train = X_train.copy()\n",
    "label_X_valid = X_valid.copy()\n",
    "\n",
    "# Apply ordinal encoder to each column with categorical data\n",
    "ordinal_encoder = OrdinalEncoder()\n",
    "label_X_train[object_cols] = ordinal_encoder.fit_transform(X_train[object_cols])\n",
    "label_X_valid[object_cols] = ordinal_encoder.transform(X_valid[object_cols])\n",
    "\n",
    "print(\"MAE from Approach 2 (Ordinal Encoding):\") \n",
    "print(score_dataset(label_X_train, label_X_valid, y_train, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping bad categorical columns first\n",
    "\n",
    "# Drop categorical columns that will not be encoded\n",
    "label_X_train = X_train.drop(bad_label_cols, axis=1)\n",
    "label_X_valid = X_valid.drop(bad_label_cols, axis=1)\n",
    "\n",
    "# Apply ordinal encoder\n",
    "ordinal_encoder = OrdinalEncoder()\n",
    "label_X_train[good_label_cols] = ordinal_encoder.fit_transform(X_train[good_label_cols])\n",
    "label_X_valid[good_label_cols] = ordinal_encoder.transform(X_valid[good_label_cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cardinality \n",
    "- Since One-Hot encoding creates a new column per value of a categorical variable. It can result in the addition of many entries for categorical variables with high cardinality. \n",
    "- Cardinality less than 10 is a good start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get number of unique entries in each column with categorical data\n",
    "object_nunique = list(map(lambda col: X_train[col].nunique(), object_cols))\n",
    "d = dict(zip(object_cols, object_nunique))\n",
    "\n",
    "# Print number of unique entries by column, in ascending order\n",
    "sorted(d.items(), key=lambda x: x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns that will be one-hot encoded\n",
    "low_cardinality_cols = [col for col in object_cols if X_train[col].nunique() < 10]\n",
    "\n",
    "# Columns that will be dropped from the dataset\n",
    "high_cardinality_cols = list(set(object_cols)-set(low_cardinality_cols))\n",
    "\n",
    "print('Categorical columns that will be one-hot encoded:', low_cardinality_cols)\n",
    "print('\\nCategorical columns that will be dropped from the dataset:', high_cardinality_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-Hot Encoding\n",
    "\n",
    "    - Set handle_unknown='ignore' to avoid errors when the validation data contains classes that aren't represented in the training data, and\n",
    "    - Set sparse=False to ensure that the encoded columns are returned as a numpy array (instead of a sparse matrix).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Apply one-hot encoder to each column with categorical data\n",
    "OH_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "OH_cols_train = pd.DataFrame(OH_encoder.fit_transform(X_train[object_cols]))\n",
    "OH_cols_valid = pd.DataFrame(OH_encoder.transform(X_valid[object_cols]))\n",
    "\n",
    "# One-hot encoding removed index; put it back\n",
    "OH_cols_train.index = X_train.index\n",
    "OH_cols_valid.index = X_valid.index\n",
    "\n",
    "# Remove categorical columns (will replace with one-hot encoding)\n",
    "num_X_train = X_train.drop(object_cols, axis=1)\n",
    "num_X_valid = X_valid.drop(object_cols, axis=1)\n",
    "\n",
    "# Add one-hot encoded columns to numerical features\n",
    "OH_X_train = pd.concat([num_X_train, OH_cols_train], axis=1)\n",
    "OH_X_valid = pd.concat([num_X_valid, OH_cols_valid], axis=1)\n",
    "\n",
    "# Ensure all columns have string type\n",
    "OH_X_train.columns = OH_X_train.columns.astype(str)\n",
    "OH_X_valid.columns = OH_X_valid.columns.astype(str)\n",
    "\n",
    "print(\"MAE from Approach 3 (One-Hot Encoding):\") \n",
    "print(score_dataset(OH_X_train, OH_X_valid, y_train, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipelines\n",
    "pipelines have some important benefits. Those include:\n",
    "\n",
    "    - Cleaner Code: Accounting for data at each step of preprocessing can get messy.\n",
    "    - Fewer Bugs: There are fewer opportunities to misapply a step or forget a preprocessing step.\n",
    "    - Easier to Productionize\n",
    "    - More Options for Model Validation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Define Preprocessing Steps\n",
    "the ColumnTransformer class bundles together different preprocessing steps. The code below:\n",
    "\n",
    "    - imputes missing values in numerical data, and\n",
    "    - imputes missing values and applies a one-hot encoding to categorical data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Preprocessing for numerical data\n",
    "numerical_transformer = SimpleImputer(strategy='constant') # For numerical data, constant fill value is 0, (can specify any constant with \"fill_value=\"\" ).\n",
    "\n",
    "# Preprocessing for categorical data\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')), # \"most frequent = mode\"\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Bundle preprocessing for numerical and categorical data\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_cols),\n",
    "        ('cat', categorical_transformer, categorical_cols)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Define the Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "model = RandomForestRegressor(n_estimators=100, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Create and Evaluate the Pipeline\n",
    "use the Pipeline class to define a pipeline that bundles the preprocessing and modeling steps. \n",
    "\n",
    "\n",
    "    - With the pipeline, we preprocess the training data and fit the model in a single line of code. \n",
    "    - With the pipeline, we supply the unprocessed features in X_valid to the predict() command, and the pipeline automatically preprocesses the features before generating predictions. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# Bundle preprocessing and modeling code in a pipeline\n",
    "my_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                              ('model', model)\n",
    "                             ])\n",
    "\n",
    "# Preprocessing of training data, fit model \n",
    "my_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Preprocessing of validation data, get predictions\n",
    "preds = my_pipeline.predict(X_valid)\n",
    "\n",
    "# Evaluate the model\n",
    "score = mean_absolute_error(y_valid, preds)\n",
    "print('MAE:', score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Restart modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Read the data\n",
    "X_full = pd.read_csv('../input/train.csv', index_col='Id')\n",
    "X_test_full = pd.read_csv('../input/test.csv', index_col='Id')\n",
    "\n",
    "# Remove rows with missing target, separate target from predictors\n",
    "X_full.dropna(axis=0, subset=['SalePrice'], inplace=True)\n",
    "y = X_full.SalePrice\n",
    "X_full.drop(['SalePrice'], axis=1, inplace=True)\n",
    "\n",
    "# Break off validation set from training data\n",
    "X_train_full, X_valid_full, y_train, y_valid = train_test_split(X_full, y, \n",
    "                                                                train_size=0.8, test_size=0.2,\n",
    "                                                                random_state=0)\n",
    "\n",
    "# \"Cardinality\" means the number of unique values in a column\n",
    "# Select categorical columns with relatively low cardinality (convenient but arbitrary)\n",
    "categorical_cols = [cname for cname in X_train_full.columns if\n",
    "                    X_train_full[cname].nunique() < 10 and \n",
    "                    X_train_full[cname].dtype == \"object\"]\n",
    "\n",
    "# Select numerical columns\n",
    "numerical_cols = [cname for cname in X_train_full.columns if \n",
    "                X_train_full[cname].dtype in ['int64', 'float64']]\n",
    "\n",
    "# Keep selected columns only\n",
    "my_cols = categorical_cols + numerical_cols\n",
    "X_train = X_train_full[my_cols].copy()\n",
    "X_valid = X_valid_full[my_cols].copy()\n",
    "X_test = X_test_full[my_cols].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# Preprocessing for numerical data\n",
    "numerical_transformer = SimpleImputer(strategy='constant')\n",
    "\n",
    "# Preprocessing for categorical data\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Bundle preprocessing for numerical and categorical data\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_cols),\n",
    "        ('cat', categorical_transformer, categorical_cols)\n",
    "    ])\n",
    "\n",
    "# Define model\n",
    "model = RandomForestRegressor(n_estimators=100, random_state=0)\n",
    "\n",
    "# Bundle preprocessing and modeling code in a pipeline\n",
    "clf = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                      ('model', model)\n",
    "                     ])\n",
    "\n",
    "# Preprocessing of training data, fit model \n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Preprocessing of validation data, get predictions\n",
    "preds = clf.predict(X_valid)\n",
    "\n",
    "print('MAE:', mean_absolute_error(y_valid, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Validation  \n",
    "In cross-validation, we run our modeling process on different subsets of the data to get multiple measures of model quality.\n",
    "\n",
    "We could begin by dividing the data into 5 pieces, each 20% of the full dataset. In this case, we say that we have broken the data into 5 \"folds\". Then, we run one experiment for each fold\n",
    "\n",
    "\n",
    "    - For small datasets, where extra computational burden isn't a big deal, you should run cross-validation.\n",
    "    - For larger datasets, a single validation set is sufficient. Your code will run faster, and you may have enough data that there's little need to re-use some of it for holdout.\n",
    "\n",
    "if your model takes a couple minutes or less to run, it's probably worth switching to cross-validation.\n",
    "\n",
    "Alternatively, you can run cross-validation and see if the scores for each experiment seem close. If each experiment yields the same results, a single validation set is probably sufficient.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Read the data\n",
    "train_data = pd.read_csv('../input/train.csv', index_col='Id')\n",
    "test_data = pd.read_csv('../input/test.csv', index_col='Id')\n",
    "\n",
    "# Remove rows with missing target, separate target from predictors\n",
    "train_data.dropna(axis=0, subset=['SalePrice'], inplace=True)\n",
    "y = train_data.SalePrice              \n",
    "train_data.drop(['SalePrice'], axis=1, inplace=True)\n",
    "\n",
    "# Select numeric columns only\n",
    "numeric_cols = [cname for cname in train_data.columns if train_data[cname].dtype in ['int64', 'float64']]\n",
    "X = train_data[numeric_cols].copy()\n",
    "X_test = test_data[numeric_cols].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "my_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', SimpleImputer()),\n",
    "    ('model', RandomForestRegressor(n_estimators=50, random_state=0))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Multiply by -1 since sklearn calculates *negative* MAE\n",
    "scores = -1 * cross_val_score(my_pipeline, X, y,\n",
    "                              cv=5, # folds of cross validation\n",
    "                              scoring='neg_mean_absolute_error')\n",
    "\n",
    "print(\"Average MAE score:\", scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_score(n_estimators):\n",
    "    \"\"\"Return the average MAE over 3 CV folds of random forest model.\n",
    "    \n",
    "    Keyword argument:\n",
    "    n_estimators -- the number of trees in the forest\n",
    "    \"\"\"\n",
    "    my_pipeline = Pipeline(steps=[\n",
    "        ('preprocessor', SimpleImputer()), \n",
    "        ('model', RandomForestRegressor(n_estimators, random_state=0))\n",
    "    ])\n",
    "    scores = -1 * cross_val_score(my_pipeline, X, y, \n",
    "                                 cv = 3, \n",
    "                                 scoring='neg_mean_absolute_error')\n",
    "    return scores.mean()\n",
    "\n",
    "\n",
    "# Itterate through various n-estimators\n",
    "results = {}\n",
    "for i in range(50, 401, 50): \n",
    "    results[i] = get_score(i) # Your code here\n",
    "\n",
    "# Save lowest result\n",
    "n_estimators_best = min(results, key=results.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize results\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.plot(list(results.keys()), list(results.values()))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to learn more about hyperparameter optimization, you're encouraged to start with grid search, which is a straightforward method for determining the best combination of parameters for a machine learning model. Thankfully, scikit-learn also contains a built-in function GridSearchCV() that can make your grid search code very efficient!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost (extreme gradient boosting)\n",
    "Ensemble methods combine the predictions of several models (e.g., several trees, in the case of random forests).\n",
    "\n",
    "XGBoost is another such ensemble method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient boosting is a method that goes through cycles to iteratively add models into an ensemble.\n",
    "\n",
    "It begins by initializing the ensemble with a single model, whose predictions can be pretty naive. (Even if its predictions are wildly inaccurate, subsequent additions to the ensemble will address those errors.)\n",
    "\n",
    "Then, we start the cycle:\n",
    "\n",
    "    First, we use the current ensemble to generate predictions for each observation in the dataset. To make a prediction, we add the predictions from all models in the ensemble.\n",
    "    These predictions are used to calculate a loss function (like mean squared error, for instance).\n",
    "    Then, we use the loss function to fit a new model that will be added to the ensemble. Specifically, we determine model parameters so that adding this new model to the ensemble will reduce the loss. (Side note: The \"gradient\" in \"gradient boosting\" refers to the fact that we'll use gradient descent on the loss function to determine the parameters in this new model.)\n",
    "    Finally, we add the new model to ensemble, and ...\n",
    "    ... repeat!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the scikit-learn API for XGBoost\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "\n",
    "\n",
    "# XGBRegressor(base_score=0.5, booster='gbtree', callbacks=None,\n",
    "#              colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1,\n",
    "#              early_stopping_rounds=None, enable_categorical=False,\n",
    "#              eval_metric=None, gamma=0, gpu_id=-1, grow_policy='depthwise',\n",
    "#              importance_type=None, interaction_constraints='',\n",
    "#              learning_rate=0.300000012, max_bin=256, max_cat_to_onehot=4,\n",
    "#              max_delta_step=0, max_depth=6, max_leaves=0, min_child_weight=1,\n",
    "#              missing=nan, monotone_constraints='()', n_estimators=100, n_jobs=0,\n",
    "#              num_parallel_tree=1, predictor='auto', random_state=0, reg_alpha=0,\n",
    "#              reg_lambda=1, ...)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- n_estimators specifies how many times to go through the modeling cycle described above. It is equal to the number of models that we include in the ensemble.\n",
    "    - Too low a value causes underfitting, which leads to inaccurate predictions on both training data and test data.\n",
    "    - Too high a value causes overfitting, which causes accurate predictions on training data, but inaccurate predictions on test data\n",
    "      - Typical values range from 100-1000, though this depends a lot on the learning_rate\n",
    "\n",
    "- early_stopping_rounds offers a way to automatically find the ideal value for n_estimators. Early stopping causes the model to stop iterating when the validation score stops improving, even if we aren't at the hard stop for n_estimators. It's smart to set a high value for n_estimators and then use early_stopping_rounds to find the optimal time to stop iterating.\n",
    "  - Since random chance sometimes causes a single round where validation scores don't improve, you need to specify a number for how many rounds of straight deterioration to allow before stopping\n",
    "    - Setting early_stopping_rounds=5 is a reasonable choice. \n",
    "      - When using early_stopping_rounds, you also need to set aside some data for calculating the validation scores - this is done by setting the eval_set parameter.\n",
    "\n",
    "- learning_rate: Instead of getting predictions by simply adding up the predictions from each component model, we can multiply the predictions from each model by a small number (known as the learning rate) before adding them in.\n",
    "  - This means each tree we add to the ensemble helps us less. So, we can set a higher value for n_estimators without overfitting. If we use early stopping, the appropriate number of trees will be determined automatically.\n",
    "    - In general, a small learning rate and large number of estimators will yield more accurate XGBoost models, though it will also take the model longer to train since it does more iterations through the cycle. As default, XGBoost sets learning_rate=0.1.\n",
    "\n",
    "\n",
    "- n_jobs: On larger datasets where runtime is a consideration, you can use parallelism to build your models faster. It's common to set the parameter n_jobs equal to the number of cores on your machine. On smaller datasets, this won't help.\n",
    "  - The resulting model won't be any better, so micro-optimizing for fitting time is typically nothing but a distraction. But, it's useful in large datasets where you would otherwise spend a long time waiting during the fit command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Read the data\n",
    "X = pd.read_csv('../input/train.csv', index_col='Id')\n",
    "X_test_full = pd.read_csv('../input/test.csv', index_col='Id')\n",
    "\n",
    "# Remove rows with missing target, separate target from predictors\n",
    "X.dropna(axis=0, subset=['SalePrice'], inplace=True)\n",
    "y = X.SalePrice              \n",
    "X.drop(['SalePrice'], axis=1, inplace=True)\n",
    "\n",
    "# Break off validation set from training data\n",
    "X_train_full, X_valid_full, y_train, y_valid = train_test_split(X, y, train_size=0.8, test_size=0.2,\n",
    "                                                                random_state=0)\n",
    "\n",
    "# \"Cardinality\" means the number of unique values in a column\n",
    "# Select categorical columns with relatively low cardinality (convenient but arbitrary)\n",
    "low_cardinality_cols = [cname for cname in X_train_full.columns if X_train_full[cname].nunique() < 10 and \n",
    "                        X_train_full[cname].dtype == \"object\"]\n",
    "\n",
    "# Select numeric columns\n",
    "numeric_cols = [cname for cname in X_train_full.columns if X_train_full[cname].dtype in ['int64', 'float64']]\n",
    "\n",
    "# Keep selected columns only\n",
    "my_cols = low_cardinality_cols + numeric_cols\n",
    "X_train = X_train_full[my_cols].copy()\n",
    "X_valid = X_valid_full[my_cols].copy()\n",
    "X_test = X_test_full[my_cols].copy()\n",
    "\n",
    "# One-hot encode the data (to shorten the code, we use pandas)\n",
    "X_train = pd.get_dummies(X_train)\n",
    "X_valid = pd.get_dummies(X_valid)\n",
    "X_test = pd.get_dummies(X_test)\n",
    "X_train, X_valid = X_train.align(X_valid, join='left', axis=1)\n",
    "X_train, X_test = X_train.align(X_test, join='left', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "my_model_1 = XGBRegressor(random_state=0)\n",
    "\n",
    "# Fit the model\n",
    "my_model_1.fit(X_train, y_train)\n",
    "\n",
    "# Get predictions\n",
    "predictions_1 = my_model_1.predict(X_valid)\n",
    "\n",
    "# Calculate MAE\n",
    "mae_1 = mean_absolute_error(predictions_1, y_valid)\n",
    "print(\"Mean Absolute Error:\" , mae_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define another model\n",
    "my_model_2 = XGBRegressor(n_estimators=1000, learning_rate=0.05)\n",
    "\n",
    "# Fit the model\n",
    "my_model_2.fit(X_train, y_train)\n",
    "\n",
    "# Get predictions\n",
    "predictions_2 = my_model_2.predict(X_valid)\n",
    "\n",
    "# Calculate MAE\n",
    "mae_2 = mean_absolute_error(predictions_2, y_valid)\n",
    "print(\"Mean Absolute Error:\" , mae_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Leakage\n",
    "Data leakage (or leakage) happens when your training data contains information about the target, but similar data will not be available when the model is used for prediction. This leads to high performance on the training set (and possibly even the validation data), but the model will perform poorly in production.\n",
    "\n",
    "There are two main types of leakage: target leakage and train-test contamination.\n",
    "\n",
    "    - Target leakage occurs when your predictors include data that will not be available at the time you make predictions. It is important to think about target leakage in terms of the timing or chronological order that data becomes available, not merely whether a feature helps make good predictions.\n",
    "    - To prevent this type of data leakage, any variable updated (or created) after the target value is realized should be excluded.\n",
    "\n",
    "    - Train-test contamination occurs when the validation data affects the preprocessing behavior\n",
    "      - For example, if you run preprocessing (like fitting an imputer for missing values) before calling train_test_split()\n",
    "      - In essence, incorporating data from the validation or test data into how you make predictions, wont allow a model to generalize well to new data.\n",
    "        - To prevent this, exclude the validation data from any type of fitting, including the fitting of preprocessing steps.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "The goal of feature engineering is simply to make your data better suited to the problem at hand\n",
    "\n",
    "This section will include:\n",
    "\n",
    "    - determine which features are the most important with mutual information\n",
    "    - invent new features in several real-world problem domains\n",
    "    - encode high-cardinality categoricals with a target encoding\n",
    "    - create segmentation features with k-means clustering\n",
    "    - decompose a dataset's variation into features with principal component analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## step 1: Feature Utility Metric\n",
    "a function measuring associations between a feature and the target.\n",
    "\n",
    "- One metric is called \"mutual information\". Mutual information is a lot like correlation in that it measures a relationship between two quantities. \n",
    "  - The advantage of mutual information is that it can detect any kind of relationship, while correlation only detects linear relationships."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mutual information\n",
    "Mutual information describes relationships in terms of uncertainty. \n",
    "\n",
    "The mutual information (MI) between two quantities is a measure of the extent to which knowledge of one quantity reduces uncertainty about the other. \n",
    "\n",
    "If you knew the value of a feature, how much more confident would you be about the target?\n",
    "\n",
    "In this case, uncertainty is measured using a quantity from information theory known as \"entropy\". The entropy of a variable means roughly: \"how many yes-or-no questions you would need to describe an occurance of that variable, on average.\" The more questions you have to ask, the more uncertain you must be about the variable. Mutual information is how many questions you expect the feature to answer about the target.\n",
    "\n",
    "Interpreting Mutual Information Scores\n",
    "\n",
    "The least possible mutual information between quantities is 0.0. When MI is zero, the quantities are independent: neither can tell you anything about the other. Conversely, in theory there's no upper bound to what MI can be. In practice though values above 2.0 or so are uncommon. (Mutual information is a logarithmic quantity, so it increases very slowly.)\n",
    "\n",
    "\n",
    "things to remember when applying mutual information:\n",
    "\n",
    "    - MI can help you to understand the relative potential of a feature as a predictor of the target, considered by itself.\n",
    "    - It's possible for a feature to be very informative when interacting with other features, but not so informative all alone. MI can't detect interactions between features. It is a univariate metric.\n",
    "    - The actual usefulness of a feature depends on the model you use it with. A feature is only useful to the extent that its relationship with the target is one your model can learn. Just because a feature has a high MI score doesn't mean your model will be able to do anything with that information. You may need to transform the feature first to expose the association.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = df.copy()\n",
    "# y = X.pop(\"price\")\n",
    "\n",
    "# # Label encoding for categoricals\n",
    "# for colname in X.select_dtypes(\"object\"):\n",
    "#     X[colname], _ = X[colname].factorize()\n",
    "\n",
    "# # All discrete features should now have integer dtypes (double-check this before using MI!)\n",
    "# discrete_features = X.dtypes == int\n",
    "\n",
    "# from sklearn.feature_selection import mutual_info_regression\n",
    "\n",
    "# def make_mi_scores(X, y, discrete_features):\n",
    "#     mi_scores = mutual_info_regression(X, y, discrete_features=discrete_features)\n",
    "#     mi_scores = pd.Series(mi_scores, name=\"MI Scores\", index=X.columns)\n",
    "#     mi_scores = mi_scores.sort_values(ascending=False)\n",
    "#     return mi_scores\n",
    "\n",
    "# mi_scores = make_mi_scores(X, y, discrete_features)\n",
    "# mi_scores[::3]  # show a few features with their MI scores\n",
    "\n",
    "# # Barplot\n",
    "# def plot_mi_scores(scores):\n",
    "#     scores = scores.sort_values(ascending=True)\n",
    "#     width = np.arange(len(scores))\n",
    "#     ticks = list(scores.index)\n",
    "#     plt.barh(width, scores)\n",
    "#     plt.yticks(width, ticks)\n",
    "#     plt.title(\"Mutual Information Scores\")\n",
    "\n",
    "\n",
    "# plt.figure(dpi=100, figsize=(8, 5))\n",
    "# plot_mi_scores(mi_scores)\n",
    "\n",
    "# # Check top MI scoring feature\n",
    "# sns.relplot(x=\"curb_weight\", y=\"price\", data=df);\n",
    "\n",
    "# # Note interaction for low scoring MI feature\n",
    "# sns.lmplot(x=\"horsepower\", y=\"price\", hue=\"fuel_type\", data=df);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility functions \n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "\n",
    "def make_mi_scores(X, y):\n",
    "    X = X.copy()\n",
    "    for colname in X.select_dtypes([\"object\", \"category\"]):\n",
    "        X[colname], _ = X[colname].factorize()\n",
    "    # All discrete features should now have integer dtypes\n",
    "    discrete_features = [pd.api.types.is_integer_dtype(t) for t in X.dtypes]\n",
    "    mi_scores = mutual_info_regression(X, y, discrete_features=discrete_features, random_state=0)\n",
    "    mi_scores = pd.Series(mi_scores, name=\"MI Scores\", index=X.columns)\n",
    "    mi_scores = mi_scores.sort_values(ascending=False)\n",
    "    return mi_scores\n",
    "\n",
    "\n",
    "def plot_mi_scores(scores):\n",
    "    scores = scores.sort_values(ascending=True)\n",
    "    width = np.arange(len(scores))\n",
    "    ticks = list(scores.index)\n",
    "    plt.barh(width, scores)\n",
    "    plt.yticks(width, ticks)\n",
    "    plt.title(\"Mutual Information Scores\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = HomeDat\n",
    "\n",
    "# Review home data features\n",
    "features = [\"YearBuilt\", \"MoSold\", \"ScreenPorch\"]\n",
    "sns.relplot(\n",
    "    x=\"value\", y=\"SalePrice\", col=\"variable\", data=df.melt(id_vars=\"SalePrice\", value_vars=features), facet_kws=dict(sharex=False),\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(axis=1)\n",
    "\n",
    "#Inspect MI scores\n",
    "X = df.copy()\n",
    "y = X.pop('SalePrice')\n",
    "\n",
    "mi_scores = make_mi_scores(X, y)\n",
    "\n",
    "print(mi_scores.head(20))\n",
    "# print(mi_scores.tail(20))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "plt.figure(dpi=100, figsize=(8, 5))\n",
    "plot_mi_scores(mi_scores.head(20))\n",
    "\n",
    "# tail\n",
    "# plot_mi_scores(mi_scores.tail(20))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect low scoring feature \"BldgType\"\n",
    "sns.catplot(x=\"BldgType\", y=\"SalePrice\", data=df, kind=\"boxen\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Investigate interaction of BldgType with GrLivArea & MoSold\n",
    "# GrLivArea  # Above ground living area\n",
    "# MoSold     # Month sold\n",
    "feature = \"GrLivArea\"\n",
    "\n",
    "sns.lmplot(\n",
    "    x=feature, y=\"SalePrice\", hue=\"BldgType\", col=\"BldgType\",\n",
    "    data=df, scatter_kws={\"edgecolor\": 'w'}, col_wrap=3, height=4,\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown, Above ground living area (GrLivArea) is strongly associated with Type of Building (BldgType)\n",
    "\n",
    "This indicates that despire low MI score of BldgType, it provides utility as a predictive feature when its interaction with GrLivArea is considered. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature = \"MoSold\"\n",
    "\n",
    "sns.lmplot(\n",
    "    x=feature, y=\"SalePrice\", hue=\"BldgType\", col=\"BldgType\",\n",
    "    data=df, scatter_kws={\"edgecolor\": 'w'}, col_wrap=3, height=4,\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create features\n",
    "\n",
    "Tips on Discovering New Features\n",
    "\n",
    "    - Understand the features. Refer to your dataset's data documentation, if available.\n",
    "    - Research the problem domain to acquire domain knowledge. If your problem is predicting house prices, do some research on real-estate for instance. Wikipedia can be a good starting point, but books and journal articles will often have the best information.\n",
    "    - Study previous work. Solution write-ups from past Kaggle competitions are a great resource.\n",
    "    - Use data visualization. Visualization can reveal pathologies in the distribution of a feature or complicated relationships that could be simplified. Be sure to visualize your dataset as you work through the feature engineering process. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Mathematically transforming variables\n",
    "\n",
    "# autos[\"stroke_ratio\"] = autos.stroke / autos.bore\n",
    "# autos[[\"stroke\", \"bore\", \"stroke_ratio\"]].head()\n",
    "\n",
    "# autos[\"displacement\"] = (\n",
    "#     np.pi * ((0.5 * autos.bore) ** 2) * autos.stroke * autos.num_of_cylinders\n",
    "# )\n",
    "\n",
    "\n",
    "# # If the feature has 0.0 values, use np.log1p (log(1+x)) instead of np.log\n",
    "# accidents[\"LogWindSpeed\"] = accidents.WindSpeed.apply(np.log1p)\n",
    "\n",
    "# # Plot a comparison\n",
    "# fig, axs = plt.subplots(1, 2, figsize=(8, 4))\n",
    "# sns.kdeplot(accidents.WindSpeed, shade=True, ax=axs[0])\n",
    "# sns.kdeplot(accidents.LogWindSpeed, shade=True, ax=axs[1]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Counting/summing related binary features\n",
    "\n",
    "# roadway_features = [\"Amenity\", \"Bump\", \"Crossing\", \"GiveWay\",\n",
    "#     \"Junction\", \"NoExit\", \"Railway\", \"Roundabout\", \"Station\", \"Stop\",\n",
    "#     \"TrafficCalming\", \"TrafficSignal\"]\n",
    "# accidents[\"RoadwayFeatures\"] = accidents[roadway_features].sum(axis=1)\n",
    "\n",
    "# accidents[roadway_features + [\"RoadwayFeatures\"]].head(10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Building-Up and Breaking-Down Features\n",
    "\n",
    "# customer[[\"Type\", \"Level\"]] = (  # Create two new features\n",
    "#     customer[\"Policy\"]           # from the Policy feature\n",
    "#     .str                         # through the string accessor\n",
    "#     .split(\" \", expand=True)     # by splitting on \" \"\n",
    "#                                  # and expanding the result into separate columns\n",
    "# )\n",
    "# customer[[\"Policy\", \"Type\", \"Level\"]].head(10)\n",
    "\n",
    "\n",
    "# autos[\"make_and_style\"] = autos[\"make\"] + \"_\" + autos[\"body_style\"]\n",
    "# autos[[\"make\", \"body_style\", \"make_and_style\"]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group transforms, which aggregate information across multiple rows grouped by some category\n",
    "\n",
    "# customer[\"AverageIncome\"] = (\n",
    "#     customer.groupby(\"State\")  # for each state\n",
    "#     [\"Income\"]                 # select the income\n",
    "#     .transform(\"mean\")         # and compute its mean\n",
    "# )\n",
    "# customer[[\"State\", \"Income\", \"AverageIncome\"]].head(10)\n",
    "\n",
    "# customer[\"StateFreq\"] = (\n",
    "#     customer.groupby(\"State\")\n",
    "#     [\"State\"]\n",
    "#     .transform(\"count\")\n",
    "#     / customer.State.count()\n",
    "# )\n",
    "\n",
    "# customer[[\"State\", \"StateFreq\"]].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you're using training and validation splits, to preserve their independence, it's best to create a grouped feature using only the training set and then join it to the validation set. We can use the validation set's merge method after creating a unique set of values with drop_duplicates on the training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create splits\n",
    "# df_train = customer.sample(frac=0.5)\n",
    "# df_valid = customer.drop(df_train.index)\n",
    "\n",
    "# # Create the average claim amount by coverage type, on the training set\n",
    "# df_train[\"AverageClaim\"] = df_train.groupby(\"Coverage\")[\"ClaimAmount\"].transform(\"mean\")\n",
    "\n",
    "# # Merge the values into the validation set\n",
    "# df_valid = df_valid.merge(\n",
    "#     df_train[[\"Coverage\", \"AverageClaim\"]].drop_duplicates(),\n",
    "#     on=\"Coverage\",\n",
    "#     how=\"left\",\n",
    "# )\n",
    "\n",
    "# df_valid[[\"Coverage\", \"AverageClaim\"]].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "    Tips on Creating Features\n",
    "    It's good to keep in mind your model's own strengths and weaknesses when creating features. Here are some guidelines:\n",
    "\n",
    "        - Linear models learn sums and differences naturally, but can't learn anything more complex.\n",
    "        - Ratios seem to be difficult for most models to learn. Ratio combinations often lead to some easy performance gains.\n",
    "        - Linear models and neural nets generally do better with normalized features. Neural nets especially need features scaled to values not too far from 0. Tree-based models (like random forests and XGBoost) can sometimes benefit from normalization, but usually much less so.\n",
    "        - Tree models can learn to approximate almost any combination of features, but when a combination is especially important they can still benefit from having it explicitly created, especially when data is limited.\n",
    "        - Counts are especially helpful for tree models, since these models don't have a natural way of aggregating information across many features at once. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = HomeDat.dropna(axis=1)\n",
    "\n",
    "X = df.copy()\n",
    "y = X.pop(\"SalePrice\") # pop() drops column\n",
    "\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_1 = pd.DataFrame()  # dataframe to hold new features\n",
    "\n",
    "X_1[\"LivLotRatio\"] = X.GrLivArea / X.LotArea\n",
    "X_1[\"Spaciousness\"] = (X['1stFlrSF'] + X['2ndFlrSF']) / X.TotRmsAbvGrd\n",
    "X_1[\"TotalOutsideSF\"] = X.WoodDeckSF + X.OpenPorchSF + X.EnclosedPorch + X['3SsnPorch'] + X.ScreenPorch\n",
    "\n",
    "X_1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you've discovered an interaction effect between a numeric feature and a categorical feature, you might want to model it explicitly using a one-hot encoding, like so:\n",
    "\n",
    "```\n",
    "# One-hot encode Categorical feature, adding a column prefix \"Cat\"\n",
    "X_new = pd.get_dummies(df.Categorical, prefix=\"Cat\")\n",
    "\n",
    "# Multiply row-by-row\n",
    "X_new = X_new.mul(df.Continuous, axis=0)\n",
    "\n",
    "# Join the new features to the feature set\n",
    "X = X.join(X_new)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accounting for interaction between continuous and categorical variables\n",
    "\n",
    "# One-hot encode BldgType. Use `prefix=\"Bldg\"` in `get_dummies`\n",
    "X_2 = pd.get_dummies(X.BldgType, prefix=\"Bldg\")\n",
    "# Multiply\n",
    "X_2 = X_2.mul(X.GrLivArea, axis=0)\n",
    "\n",
    "X_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count instances of the following porch types as a new variable\n",
    "# WoodDeckSF\n",
    "# OpenPorchSF\n",
    "# EnclosedPorch\n",
    "# Threeseasonporch\n",
    "# ScreenPorch\n",
    "\n",
    "X_3 = pd.DataFrame()\n",
    "\n",
    "porch_features = [\"WoodDeckSF\", \"OpenPorchSF\", \"EnclosedPorch\", \"3SsnPorch\",\"ScreenPorch\"]\n",
    "\n",
    "X_3[\"PorchTypes\"] = (X[porch_features] > 0).sum(axis=1)\n",
    "\n",
    "X_3\n",
    "\n",
    "# # Alternaively, use pandas gt() function\n",
    "# X_3[\"PorchTypes\"] = df[[\"WoodDeckSF\", \"OpenPorchSF\", \"EnclosedPorch\", \"3SsnPorch\", \"ScreenPorch\",]].gt(0.0).sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Break Down a Categorical Feature\n",
    "\n",
    "# MSSubClass describes the type of a dwelling:\n",
    "\n",
    "X.MSSubClass.unique()\n",
    "\n",
    "# MSSubClass: Identifies the type of dwelling involved in the sale.\t\n",
    "\n",
    "#         20\t1-STORY 1946 & NEWER ALL STYLES\n",
    "#         30\t1-STORY 1945 & OLDER\n",
    "#         40\t1-STORY W/FINISHED ATTIC ALL AGES\n",
    "#         45\t1-1/2 STORY - UNFINISHED ALL AGES\n",
    "#         50\t1-1/2 STORY FINISHED ALL AGES\n",
    "#         60\t2-STORY 1946 & NEWER\n",
    "#         70\t2-STORY 1945 & OLDER\n",
    "#         75\t2-1/2 STORY ALL AGES\n",
    "#         80\tSPLIT OR MULTI-LEVEL\n",
    "#         85\tSPLIT FOYER\n",
    "#         90\tDUPLEX - ALL STYLES AND AGES\n",
    "#        120\t1-STORY PUD (Planned Unit Development) - 1946 & NEWER\n",
    "#        150\t1-1/2 STORY PUD - ALL AGES\n",
    "#        160\t2-STORY PUD - 1946 & NEWER\n",
    "#        180\tPUD - MULTILEVEL - INCL SPLIT LEV/FOYER\n",
    "#        190\t2 FAMILY CONVERSION - ALL STYLES AND AGES\n",
    "\n",
    "# # Wont work because of keys\n",
    "# X_4 = pd.DataFrame()\n",
    "\n",
    "# # YOUR CODE HERE\n",
    "# X_4[[\"MSClass\"]] = (df[\"MSSubClass\"].str.split(\"_\", n=1, expand=True)[[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a feature MedNhbdArea that describes the median of GrLivArea grouped on Neighborhood. Because the value of a home often depends on how it compares to typical homes in its neighborhood\n",
    "X_5 = pd.DataFrame()\n",
    "\n",
    "X_5[\"MedNhbdArea\"] = (df.groupby('Neighborhood')['GrLivArea'].transform(\"median\"))\n",
    "\n",
    "X_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score model with new features \n",
    "X_new = X.join([X_1, X_2, X_3, X_5])\n",
    "\n",
    "score_dataset(X_new, y) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering (Unsupervised)\n",
    "Clustering simply means the assigning of data points to groups based upon how similar the points are to each other.\n",
    "\n",
    "- Applied to a single real-valued feature, clustering acts like a traditional \"binning\" or \"discretization\" transform. \n",
    "\n",
    "- On multiple features, it's like \"multi-dimensional binning\" (sometimes called vector quantization)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K-Means\n",
    "K-means clustering measures similarity using ordinary straight-line distance (Euclidean distance, in other words). It creates clusters by placing a number of points, called centroids, inside the feature-space. Each point in the dataset is assigned to the cluster of whichever centroid it's closest to. The \"k\" in \"k-means\" is how many centroids (that is, clusters) it creates. You define the k yourself.\n",
    "\n",
    "- imagine each centroid capturing points through a sequence of radiating circles. When sets of circles from competing centroids overlap they form a line. The result is what's called a `Voronoi tessallation`. The tessallation shows you to what clusters future data will be assigned; the tessallation is essentially what k-means learns from its training data.\n",
    "\n",
    "how the k-means algorithm learns the clusters and what that means for feature engineering. We'll focus on three parameters from scikit-learn's implementation: n_clusters, max_iter, and n_init.\n",
    "\n",
    "It's a simple two-step process. The algorithm starts by randomly initializing some predefined number (n_clusters) of centroids. It then iterates over these two operations:\n",
    "\n",
    "    - assign points to the nearest cluster centroid\n",
    "    - move each centroid to minimize the distance to its points\n",
    "\n",
    "It iterates over these two steps until the centroids aren't moving anymore, or until some maximum number of iterations has passed (max_iter).\n",
    "\n",
    "It often happens that the initial random position of the centroids ends in a poor clustering. For this reason the algorithm repeats a number of times (n_init) and returns the clustering that has the least total distance between each point and its centroid, the optimal clustering.\n",
    "\n",
    "`good idea to scale values if they contain extremes`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "# df = pd.read_csv(\"../input/fe-course-data/housing.csv\")\n",
    "# X = df.loc[:, [\"MedInc\", \"Latitude\", \"Longitude\"]]\n",
    "\n",
    "# # Create cluster feature\n",
    "# kmeans = KMeans(n_clusters=6)\n",
    "# X[\"Cluster\"] = kmeans.fit_predict(X)\n",
    "# X[\"Cluster\"] = X[\"Cluster\"].astype(\"category\")\n",
    "\n",
    "# # Plots to insepct accurace\n",
    "# sns.relplot(\n",
    "#     x=\"Longitude\", y=\"Latitude\", hue=\"Cluster\", data=X, height=6,\n",
    "# );\n",
    "\n",
    "# X[\"MedHouseVal\"] = df[\"MedHouseVal\"]\n",
    "# sns.catplot(x=\"MedHouseVal\", y=\"Cluster\", data=X, kind=\"boxen\", height=6);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "def score_dataset(X, y, model=XGBRegressor()):\n",
    "    # Label encoding for categoricals\n",
    "    for colname in X.select_dtypes([\"category\", \"object\"]):\n",
    "        X[colname], _ = X[colname].factorize()\n",
    "    # Metric for Housing competition is RMSLE (Root Mean Squared Log Error)\n",
    "    score = cross_val_score(\n",
    "        model, X, y, cv=5, scoring=\"neg_mean_squared_log_error\",\n",
    "    )\n",
    "    score = -1 * score.mean()\n",
    "    score = np.sqrt(score)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = HomeDat.dropna(axis=1)\n",
    "\n",
    "X = df.copy()\n",
    "y = X.pop(\"SalePrice\") # pop() drops column\n",
    "\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "X = df.copy()\n",
    "y = X.pop(\"SalePrice\")\n",
    "\n",
    "\n",
    "# YOUR CODE HERE: Define a list of the features to be used for the clustering\n",
    "features = [\"LotArea\", \"TotalBsmtSF\", \"1stFlrSF\", \"2ndFlrSF\", \"GrLivArea\"]\n",
    "\n",
    "\n",
    "# Standardize\n",
    "X_scaled = X.loc[:, features]\n",
    "X_scaled = (X_scaled - X_scaled.mean(axis=0)) / X_scaled.std(axis=0)\n",
    "\n",
    "\n",
    "# YOUR CODE HERE: Fit the KMeans model to X_scaled and create the cluster labels\n",
    "kmeans = KMeans(n_clusters=10, n_init=10, random_state=0)\n",
    "X[\"Cluster\"] = kmeans.fit_predict(X_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xy = X.copy()\n",
    "Xy[\"Cluster\"] = Xy.Cluster.astype(\"category\")\n",
    "Xy[\"SalePrice\"] = y\n",
    "sns.relplot(\n",
    "    x=\"value\", y=\"SalePrice\", hue=\"Cluster\", col=\"variable\",\n",
    "    height=4, aspect=1, facet_kws={'sharex': False}, col_wrap=3,\n",
    "    data=Xy.melt(\n",
    "        value_vars=features, id_vars=[\"SalePrice\", \"Cluster\"],\n",
    "    ),\n",
    ");\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_dataset(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cluster Distance feature\n",
    "\n",
    "The k-means algorithm offers an alternative way of creating features. Instead of labelling each feature with the nearest cluster centroid, it can measure the distance from a point to all the centroids and return those distances as features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=10, n_init=10, random_state=0)\n",
    "\n",
    "\n",
    "# Create the cluster-distance features using `fit_transform`\n",
    "X_cd = kmeans.fit_transform(X_scaled)\n",
    "\n",
    "\n",
    "# Label features and join to dataset\n",
    "X_cd = pd.DataFrame(X_cd, columns=[f\"Centroid_{i}\" for i in range(X_cd.shape[1])])\n",
    "X = X.join(X_cd)\n",
    "\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_dataset(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Principal Component Analysis (PCA)\n",
    "Just like clustering is a partitioning of the dataset based on proximity, you could think of PCA as a partitioning of the variation in the data. PCA is a great tool to help you discover important relationships in the data and can also be used to create more informative features.\n",
    "\n",
    "\n",
    "`Technical note: PCA is typically applied to standardized data. With standardized data \"variation\" means \"correlation\". With unstandardized data \"variation\" means \"covariance\". All data in this course will be standardized before applying PCA.`\n",
    "\n",
    "The new features PCA constructs are actually just linear combinations (weighted sums) of the original features\n",
    "\n",
    "These new features are called the principal components of the data. The weights themselves are called loadings.\n",
    "\n",
    "A component's loadings tell us what variation it expresses through signs and magnitudes\n",
    "\n",
    "PCA also tells us the amount of variation in each component through each component's `percent of explained variance`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two ways you could use PCA for feature engineering.\n",
    "\n",
    "The first way is to use it as a descriptive technique. Since the components tell you about the variation, you could compute the MI scores for the components and see what kind of variation is most predictive of your target. That could give you ideas for kinds of features to create -- a product of 'Height' and 'Diameter' if 'Size' is important, say, or a ratio of 'Height' and 'Diameter' if Shape is important. You could even try clustering on one or more of the high-scoring components.\n",
    "\n",
    "The second way is to use the components themselves as features. Because the components expose the variational structure of the data directly, they can often be more informative than the original features. Here are some use-cases:\n",
    "\n",
    "    - Dimensionality reduction: When your features are highly redundant (multicollinear, specifically), PCA will partition out the redundancy into one or more near-zero variance components, which you can then drop since they will contain little or no information.\n",
    "    - `Anomaly detection`: Unusual variation, not apparent from the original features, will often show up in the low-variance components. These components could be highly informative in an anomaly or outlier detection task.\n",
    "    - `Noise reduction`: A collection of sensor readings will often share some common background noise. PCA can sometimes collect the (informative) signal into a smaller number of features while leaving the noise alone, thus boosting the signal-to-noise ratio.\n",
    "    - `Decorrelation`: Some ML algorithms struggle with highly-correlated features. PCA transforms correlated features into uncorrelated components, which could be easier for your algorithm to work with.\n",
    "\n",
    "PCA basically gives you direct access to the correlational structure of your data.\n",
    "\n",
    "\n",
    "     PCA Best Practices\n",
    "    There are a few things to keep in mind when applying PCA:\n",
    "\n",
    "        PCA only works with numeric features, like continuous quantities or counts.\n",
    "        PCA is sensitive to scale. It's good practice to standardize your data before applying PCA, unless you know you have good reason not to.\n",
    "        Consider removing or constraining outliers, since they can have an undue influence on the results. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features = [\"highway_mpg\", \"engine_size\", \"horsepower\", \"curb_weight\"]\n",
    "\n",
    "# X = df.copy()\n",
    "# y = X.pop('price')\n",
    "# X = X.loc[:, features]\n",
    "\n",
    "# # Standardize\n",
    "# X_scaled = (X - X.mean(axis=0)) / X.std(axis=0)\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# # Create principal components\n",
    "# pca = PCA()\n",
    "# X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# # Convert to dataframe\n",
    "# component_names = [f\"PC{i+1}\" for i in range(X_pca.shape[1])]\n",
    "# X_pca = pd.DataFrame(X_pca, columns=component_names)\n",
    "\n",
    "# X_pca.head()\n",
    "\n",
    "# loadings = pd.DataFrame(\n",
    "#     pca.components_.T,  # transpose the matrix of loadings\n",
    "#     columns=component_names,  # so the columns are the principal components\n",
    "#     index=X.columns,  # and the rows are the original features\n",
    "# )\n",
    "# loadings\n",
    "\n",
    "# # Look at explained variance\n",
    "# plot_variance(pca);\n",
    "\n",
    "\n",
    "# # Examine components with low variance through MI score\n",
    "# mi_scores = make_mi_scores(X_pca, y, discrete_features=False)\n",
    "# mi_scores\n",
    "\n",
    "# # Note any contrasts \n",
    "# # Show dataframe sorted by PC3\n",
    "# idx = X_pca[\"PC3\"].sort_values(ascending=False).index\n",
    "# cols = [\"make\", \"body_style\", \"horsepower\", \"curb_weight\"]\n",
    "# df.loc[idx, cols]\n",
    "\n",
    "# # To express this contrast, let's create a new ratio feature\n",
    "# df[\"sports_or_wagon\"] = X.curb_weight / X.horsepower\n",
    "# sns.regplot(x=\"sports_or_wagon\", y='price', data=df, order=2);\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "def apply_pca(X, standardize=True):\n",
    "    # Standardize\n",
    "    if standardize:\n",
    "        X = (X - X.mean(axis=0)) / X.std(axis=0)\n",
    "    # Create principal components\n",
    "    pca = PCA()\n",
    "    X_pca = pca.fit_transform(X)\n",
    "    # Convert to dataframe\n",
    "    component_names = [f\"PC{i+1}\" for i in range(X_pca.shape[1])]\n",
    "    X_pca = pd.DataFrame(X_pca, columns=component_names)\n",
    "    # Create loadings\n",
    "    loadings = pd.DataFrame(\n",
    "        pca.components_.T,  # transpose the matrix of loadings\n",
    "        columns=component_names,  # so the columns are the principal components\n",
    "        index=X.columns,  # and the rows are the original features\n",
    "    )\n",
    "    return pca, X_pca, loadings\n",
    "\n",
    "\n",
    "def plot_variance(pca, width=8, dpi=100):\n",
    "    # Create figure\n",
    "    fig, axs = plt.subplots(1, 2)\n",
    "    n = pca.n_components_\n",
    "    grid = np.arange(1, n + 1)\n",
    "    # Explained variance\n",
    "    evr = pca.explained_variance_ratio_\n",
    "    axs[0].bar(grid, evr)\n",
    "    axs[0].set(\n",
    "        xlabel=\"Component\", title=\"% Explained Variance\", ylim=(0.0, 1.0)\n",
    "    )\n",
    "    # Cumulative Variance\n",
    "    cv = np.cumsum(evr)\n",
    "    axs[1].plot(np.r_[0, grid], np.r_[0, cv], \"o-\")\n",
    "    axs[1].set(\n",
    "        xlabel=\"Component\", title=\"% Cumulative Variance\", ylim=(0.0, 1.0)\n",
    "    )\n",
    "    # Set up figure\n",
    "    fig.set(figwidth=8, dpi=100)\n",
    "    return axs\n",
    "\n",
    "\n",
    "def make_mi_scores(X, y):\n",
    "    X = X.copy()\n",
    "    for colname in X.select_dtypes([\"object\", \"category\"]):\n",
    "        X[colname], _ = X[colname].factorize()\n",
    "    # All discrete features should now have integer dtypes\n",
    "    discrete_features = [pd.api.types.is_integer_dtype(t) for t in X.dtypes]\n",
    "    mi_scores = mutual_info_regression(X, y, discrete_features=discrete_features, random_state=0)\n",
    "    mi_scores = pd.Series(mi_scores, name=\"MI Scores\", index=X.columns)\n",
    "    mi_scores = mi_scores.sort_values(ascending=False)\n",
    "    return mi_scores\n",
    "\n",
    "\n",
    "def score_dataset(X, y, model=XGBRegressor()):\n",
    "    # Label encoding for categoricals\n",
    "    for colname in X.select_dtypes([\"category\", \"object\"]):\n",
    "        X[colname], _ = X[colname].factorize()\n",
    "    # Metric for Housing competition is RMSLE (Root Mean Squared Log Error)\n",
    "    score = cross_val_score(\n",
    "        model, X, y, cv=5, scoring=\"neg_mean_squared_log_error\",\n",
    "    )\n",
    "    score = -1 * score.mean()\n",
    "    score = np.sqrt(score)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = HomeDat.dropna(axis=1)\n",
    "\n",
    "X = df.copy()\n",
    "y = X.pop(\"SalePrice\") # pop() drops column\n",
    "\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\n",
    "    \"GarageArea\",\n",
    "    \"YearRemodAdd\",\n",
    "    \"TotalBsmtSF\",\n",
    "    \"GrLivArea\",\n",
    "]\n",
    "\n",
    "# Determine correlations to Saleprice\n",
    "print(\"Correlation with SalePrice:\\n\")\n",
    "print(df[features].corrwith(df.SalePrice))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply PCA and extract the loadings.\n",
    "X = df.copy()\n",
    "y = X.pop(\"SalePrice\")\n",
    "X = X.loc[:, features]\n",
    "\n",
    "# `apply_pca`, defined above, reproduces the code from the tutorial\n",
    "pca, X_pca, loadings = apply_pca(X)\n",
    "print(loadings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first component, PC1, seems to be a kind of \"size\" component, similar to what we saw in the tutorial: all of the features have the same sign (positive), indicating that this component is describing a contrast between houses having large values and houses having small values for these features.\n",
    "\n",
    "The interpretation of the third component PC3 is a little trickier. The features GarageArea and YearRemodAdd both have near-zero loadings, so let's ignore those. This component is mostly about TotalBsmtSF and GrLivArea. It describes a contrast between houses with a lot of living area but small (or non-existant) basements, and the opposite: small houses with large basements.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create new features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA Use Cases\n",
    "\n",
    "# Option 1: New feature Inspired by loadings\n",
    "X = df.copy()\n",
    "y = X.pop(\"SalePrice\")\n",
    "\n",
    "X[\"Feature1\"] = X.GrLivArea + X.TotalBsmtSF\n",
    "X[\"Feature2\"] = X.YearRemodAdd * X.TotalBsmtSF\n",
    "\n",
    "score = score_dataset(X, y)\n",
    "print(f\"Your score: {score:.5f} RMSLE\")\n",
    "\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 2: Uses components as new features\n",
    "X = df.copy()\n",
    "y = X.pop(\"SalePrice\")\n",
    "\n",
    "X = X.join(X_pca)\n",
    "score = score_dataset(X, y)\n",
    "print(f\"Your score: {score:.5f} RMSLE\")\n",
    "\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Outlier Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(\n",
    "    y=\"value\",\n",
    "    col=\"variable\",\n",
    "    data=X_pca.melt(),\n",
    "    kind='boxen',\n",
    "    sharey=False,\n",
    "    col_wrap=2,\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #  See outliers in PC1\n",
    "# component = \"PC2\"\n",
    "\n",
    "# idx = X_pca[component].sort_values(ascending=False).index\n",
    "# X.loc[idx, [\"SalePrice\", \"Neighborhood\", \"SaleCondition\"] + features]\n",
    "\n",
    "# Notice that there are several dwellings listed as Partial sales in the Edwards neighborhood that stand out. A partial sale is what occurs when there are multiple owners of a property and one or more of them sell their \"partial\" ownership of the property.\n",
    "\n",
    "# These kinds of sales are often happen during the settlement of a family estate or the dissolution of a business and aren't advertised publicly. If you were trying to predict the value of a house on the open market, you would probably be justified in removing sales like these from your dataset -- they are truly outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Target Encoding\n",
    "\n",
    "a method of encoding categories as numbers, like one-hot or label encoding, with the difference that it also uses the target to create the encoding. This makes it what we call a supervised feature engineering technique.\n",
    "\n",
    "A target encoding is any kind of encoding that replaces a feature's categories with some number derived from the target.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean encoding\n",
    "\n",
    "A simple and effective version is to apply a group aggregation like the mean. \n",
    "\n",
    "This kind of target encoding is sometimes called a mean encoding. Applied to a binary target, it's also called bin counting. (Other names you might come across include: likelihood encoding, impact encoding, and leave-one-out encoding.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Encode with mean of categorical variable\n",
    "# autos[\"make_encoded\"] = autos.groupby(\"make\")[\"price\"].transform(\"mean\")\n",
    "# autos[[\"make\", \"price\", \"make_encoded\"]].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Smoothing\n",
    "\n",
    "Mean encoding presents a couple of problems, however. First are unknown categories. Target encodings create a special risk of overfitting, which means they need to be trained on an independent \"encoding\" split. When you join the encoding to future splits, Pandas will fill in missing values for any categories not present in the encoding split. These missing values you would have to impute somehow.\n",
    "\n",
    "Second are rare categories. When a category only occurs a few times in the dataset, any statistics calculated on its group are unlikely to be very accurate. In the Automobiles dataset, the mercurcy make only occurs once. The \"mean\" price we calculated is just the price of that one vehicle, which might not be very representative of any Mercuries we might see in the future. Target encoding rare categories can make overfitting more likely.\n",
    "\n",
    "A solution to these problems is to add smoothing. The idea is to blend the in-category average with the overall average. Rare categories get less weight on their category average, while missing categories just get the overall average.\n",
    "\n",
    "In pseudocode:\n",
    "\n",
    "`encoding = weight * in_category + (1 - weight) * overall`\n",
    "\n",
    "where weight is a value between 0 and 1 calculated from the category frequency.\n",
    "\n",
    "An easy way to determine the value for weight is to compute an m-estimate:\n",
    "\n",
    "`weight = n / (n + m)`\n",
    "\n",
    "where n is the total number of times that category occurs in the data. The parameter m determines the \"smoothing factor\". Larger values of m put more weight on the overall estimate.\n",
    "\n",
    "     Use Cases for Target Encoding\n",
    "    Target encoding is great for:\n",
    "\n",
    "        High-cardinality features: A feature with a large number of categories can be troublesome to encode: a one-hot encoding would generate too many features and alternatives, like a label encoding, might not be appropriate for that feature. A target encoding derives numbers for the categories using the feature's most important property: its relationship with the target.\n",
    "        Domain-motivated features: From prior experience, you might suspect that a categorical feature should be important even if it scored poorly with a feature metric. A target encoding can help reveal a feature's true informativeness. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Test train split pre-encoding (as we are aggregating things)\n",
    "# X = df.copy()\n",
    "# y = X.pop('Rating')\n",
    "\n",
    "# X_encode = X.sample(frac=0.25)\n",
    "# y_encode = y[X_encode.index]\n",
    "# X_pretrain = X.drop(X_encode.index)\n",
    "# y_train = y[X_pretrain.index]\n",
    "\n",
    "from category_encoders import MEstimateEncoder\n",
    "\n",
    "# # Create the encoder instance. Choose m to control noise.\n",
    "# encoder = MEstimateEncoder(cols=[\"Zipcode\"], m=5.0)\n",
    "\n",
    "# # Fit the encoder on the encoding split.\n",
    "# encoder.fit(X_encode, y_encode)\n",
    "\n",
    "# # Encode the Zipcode column to create the final training data\n",
    "# X_train = encoder.transform(X_pretrain)\n",
    "\n",
    "# # Compare encoded value to target value\n",
    "# plt.figure(dpi=90)\n",
    "# ax = sns.distplot(y, kde=False, norm_hist=True)\n",
    "# ax = sns.kdeplot(X_train.Zipcode, color='r', ax=ax)\n",
    "# ax.set_xlabel(\"Rating\")\n",
    "# ax.legend(labels=['Zipcode', 'Rating']);\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from category_encoders import MEstimateEncoder\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "\n",
    "def score_dataset(X, y, model=XGBRegressor()):\n",
    "    # Label encoding for categoricals\n",
    "    for colname in X.select_dtypes([\"category\", \"object\"]):\n",
    "        X[colname], _ = X[colname].factorize()\n",
    "    # Metric for Housing competition is RMSLE (Root Mean Squared Log Error)\n",
    "    score = cross_val_score(\n",
    "        model, X, y, cv=5, scoring=\"neg_mean_squared_log_error\",\n",
    "    )\n",
    "    score = -1 * score.mean()\n",
    "    score = np.sqrt(score)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = HomeDat.dropna(axis=1)\n",
    "\n",
    "X = df.copy()\n",
    "y = X.pop(\"SalePrice\") # pop() drops column\n",
    "\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select_dtypes([\"object\"]).nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"SaleType\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Features to encode\n",
    "\n",
    "    The Neighborhood feature looks promising. It has the most categories of any feature, and several categories are rare. Others that could be worth considering are SaleType, MSSubClass, Exterior1st, Exterior2nd. In fact, almost any of the nominal features would be worth trying because of the prevalence of rare categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To avoid overfitting, we need to fit the encoder on data heldout from the training set.\n",
    "# Encoding split\n",
    "X_encode = df.sample(frac=0.20, random_state=0)\n",
    "y_encode = X_encode.pop(\"SalePrice\")\n",
    "\n",
    "# Training split\n",
    "X_pretrain = df.drop(X_encode.index)\n",
    "y_train = X_pretrain.pop(\"SalePrice\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a set of features to encode and a value for m (arbitrarily chosen)\n",
    "encoder = MEstimateEncoder(cols=[\"Neighborhood\"], m=1.0)\n",
    "\n",
    "\n",
    "# Fit the encoder on the encoding split\n",
    "encoder.fit(X_encode, y_encode)\n",
    "\n",
    "# Encode the training split\n",
    "X_train = encoder.transform(X_pretrain, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare\n",
    "feature = encoder.cols\n",
    "\n",
    "plt.figure(dpi=90)\n",
    "ax = sns.distplot(y_train, kde=True, hist=False)\n",
    "ax = sns.distplot(X_train[feature], color='r', ax=ax, hist=True, kde=False, norm_hist=True)\n",
    "ax.set_xlabel(\"SalePrice\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score encoding\n",
    "X = df.copy()\n",
    "y = X.pop(\"SalePrice\")\n",
    "score_base = score_dataset(X, y)\n",
    "score_new = score_dataset(X_train, y_train)\n",
    "\n",
    "print(f\"Baseline Score: {score_base:.4f} RMSLE\")\n",
    "print(f\"Score with Encoding: {score_new:.4f} RMSLE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References\n",
    "\n",
    "Here are some great resources you might like to consult for more information. They all played a part in shaping this course:\n",
    "\n",
    "    The Art of Feature Engineering, a book by Pablo Duboue.\n",
    "    An Empirical Analysis of Feature Engineering for Predictive Modeling, an article by Jeff Heaton.\n",
    "    Feature Engineering for Machine Learning, a book by Alice Zheng and Amanda Casari. The tutorial on clustering was inspired by this excellent book.\n",
    "    Feature Engineering and Selection, a book by Max Kuhn and Kjell Johnson.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Draft Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing values\n",
    "# Fill with mean        \n",
    "# HomeDat[numerical_columns] = HomeDat[numerical_columns].fillna(HomeDat[numerical_columns].mean())\n",
    "\n",
    "# Fill with specfied value      \n",
    "# HomeDat[categorical_columns] = HomeDat[categorical_columns].fillna(\"Unknown\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform Categorical Variables\n",
    "- One-hot encoding\n",
    "- Label encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Convert categorical variables using one-hot encoding or label encoding\n",
    "# HomeDat_encoded = pd.get_dummies(HomeDat[categorical_columns], drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scale Numerical Variables\n",
    "- Standardization\n",
    "- Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Initialize a scaler and fit_transform the numerical columns\n",
    "# scaler = StandardScaler()\n",
    "# HomeDat_scaled = pd.DataFrame(scaler.fit_transform(HomeDat[numerical_columns]), columns=numerical_columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outliers and Skewness\n",
    "- In numerical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Check skewness\n",
    "# skewness = HomeDat[numerical_columns].skew()\n",
    "# print(f\"Skewness in numerical columns: \\n{skewness}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Pseudolog Transformation\n",
    "# for col in numerical_columns:\n",
    "#     if abs(skewness[col]) > 1:\n",
    "#         HomeDat_scaled[col] = np.log1p(HomeDat_scaled[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Combine Processed Data\n",
    "# # Combine the scaled numerical and encoded categorical columns\n",
    "# HomeDat_prepared = pd.concat([HomeDat_scaled, HomeDat_encoded], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Predictor and outcome vaiarbles\n",
    "# X = HomeDat_prepared.drop(columns=['SalePrice'])\n",
    "# y = HomeDat['SalePrice']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example of interaction features: Square footage * number of rooms\n",
    "# HomeDat['TotalRooms_SquareFootage'] = HomeDat['TotRmsAbvGrd'] * HomeDat['GrLivArea']\n",
    "\n",
    "# # Example of polynomial features: Adding squares of features\n",
    "# from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "# poly = PolynomialFeatures(degree=2, include_bias=False)\n",
    "# poly_features = poly.fit_transform(HomeDat[['GrLivArea', 'LotArea']])\n",
    "\n",
    "# # Convert to DataFrame and join with original dataset\n",
    "# poly_df = pd.DataFrame(poly_features, columns=poly.get_feature_names_out(['GrLivArea', 'LotArea']))\n",
    "# HomeDat = HomeDat.join(poly_df)\n",
    "\n",
    "# # Log transform a highly skewed feature (e.g., LotArea)\n",
    "# HomeDat['LotArea_Log'] = np.log1p(HomeDat['LotArea'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Correlation matrix\n",
    "# corr_matrix = HomeDat.corr()\n",
    "\n",
    "# # Visualize the correlation matrix\n",
    "# plt.figure(figsize=(12, 8))\n",
    "# sns.heatmap(corr_matrix, annot=True, cmap='coolwarm')\n",
    "# plt.title('Correlation Matrix')\n",
    "# plt.show()\n",
    "\n",
    "# # Keep features with high correlation to SalePrice (e.g., above 0.5)\n",
    "# high_corr_features = corr_matrix.index[corr_matrix['SalePrice'].abs() > 0.5]\n",
    "# print(f\"Highly correlated features with SalePrice: {high_corr_features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Prepare features and target\n",
    "# X = HomeDat.drop(columns=['SalePrice'])  # Remove the target variable\n",
    "# y = HomeDat['SalePrice']\n",
    "\n",
    "# # Split the data\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# # Train a RandomForest model to get feature importance\n",
    "# rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "# rf.fit(X_train, y_train)\n",
    "\n",
    "# # Get feature importance\n",
    "# feature_importance = pd.DataFrame({\n",
    "#     'Feature': X_train.columns,\n",
    "#     'Importance': rf.feature_importances_\n",
    "# }).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# # Display the top 10 most important features\n",
    "# print(feature_importance.head(10))\n",
    "\n",
    "# # Visualize feature importance\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# sns.barplot(x='Importance', y='Feature', data=feature_importance.head(10))\n",
    "# plt.title('Top 10 Important Features')\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.feature_selection import RFE\n",
    "# from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# # Initialize a model (Linear Regression, but can use others)\n",
    "# model = LinearRegression()\n",
    "\n",
    "# # Recursive Feature Elimination\n",
    "# rfe = RFE(estimator=model, n_features_to_select=10)  # Select top 10 features\n",
    "# rfe.fit(X_train, y_train)\n",
    "\n",
    "# # Get the selected features\n",
    "# selected_features = X_train.columns[rfe.support_]\n",
    "# print(f\"Selected features: {selected_features}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.feature_selection import SelectKBest, f_regression\n",
    "\n",
    "# # Select the top 10 features based on correlation with SalePrice\n",
    "# select_k_best = SelectKBest(score_func=f_regression, k=10)\n",
    "# fit = select_k_best.fit(X_train, y_train)\n",
    "\n",
    "# # Get the selected features\n",
    "# best_features = X_train.columns[select_k_best.get_support()]\n",
    "# print(f\"Top 10 best features: {best_features}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Multicolinearity\n",
    "\n",
    "# from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "# # Calculate VIF for each feature\n",
    "# vif = pd.DataFrame()\n",
    "# vif[\"Features\"] = X.columns\n",
    "# vif[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
    "\n",
    "# print(vif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Dimensionality Reduction\n",
    "# from sklearn.decomposition import PCA\n",
    "\n",
    "# pca = PCA(n_components=10)\n",
    "# X_pca = pca.fit_transform(X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create X by selecting numerical predictors:\n",
    "\n",
    "    - features = home_data.select_dtypes(exclude=['object', 'category']).drop(columns=['SalePrice', 'Id']).columns\n",
    "    - features = ['LotArea', 'YearBuilt', '1stFlrSF', '2ndFlrSF', 'FullBath', 'BedroomAbvGr', 'TotRmsAbvGrd']\n",
    "\n",
    "Select columns corresponding to features, and preview the data:\n",
    "\n",
    "    - X = home_data[features]\n",
    "\n",
    "Split into validation and training data:\n",
    "\n",
    "    - train_X, val_X, train_y, val_y = train_test_split(X, y, random_state=1)\n",
    "\n",
    "Define a random forest model:\n",
    "\n",
    "    - rf_model = RandomForestRegressor(random_state=1)\n",
    "    - rf_model.fit(train_X, train_y)\n",
    "    - rf_val_predictions = rf_model.predict(val_X)\n",
    "    - rf_val_mae = mean_absolute_error(rf_val_predictions, val_y)\n",
    "\n",
    "    - print(\"Validation MAE for Random Forest Model: {:,.0f}\".format(rf_val_mae))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wrangle Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "explain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "explain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "explain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discoveries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "explain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Future Directions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "explain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show Session Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import session_info\n",
    "session_info.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Session Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace spaces in notebook title with underscores\n",
    "filename = Notebook_title.replace(\" \", \"_\") + \"_requirements.txt\"\n",
    "\n",
    "# Run the pip freeze command and save the output txt file\n",
    "!pip freeze > $filename"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
